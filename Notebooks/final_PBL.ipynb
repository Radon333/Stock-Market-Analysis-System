{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3.6 (tensorflow)",
      "language": "python",
      "name": "tensorflow"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "t81_558_class_04_3_regression.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzmflFpUbW-X"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_04_3_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJ8c4gvfbW-h",
        "outputId": "e47e76db-72d3-4c34-a012-5d446c0e0b47"
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note: using Google CoLab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9QmXSwabW-j"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import zscore\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "df = pd.read_csv(\n",
        "    \"https://raw.githubusercontent.com/Saketspradhan/Stock-Market-Analysis-System/main/Datasets/PBL_train_v2.csv\",\n",
        "    na_values=['NA','?'])\n",
        "\n",
        "x_columns = df.columns.drop('CMP').drop('Name')\n",
        "x = df[x_columns].values\n",
        "y = df['CMP'].values\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(    \n",
        "    x, y, test_size=0.25, random_state=42)\n",
        "\n",
        "x_train = np.asarray(x_train).astype('float32')\n",
        "x_test = np.asarray(x_test).astype('float32')\n",
        "y_train = np.asarray(y_train).astype('float32')\n",
        "y_test = np.asarray(y_test).astype('float32')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSRMgVa1bW-k",
        "outputId": "97aced43-19c4-46e0-9436-4b6ac20e10d8"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(128, kernel_initializer='normal', input_dim=x.shape[1], activation='relu')) # Hidden 1\n",
        "\n",
        "model.add(Dense(64, kernel_initializer='normal', activation='relu')) # Hidden 2\n",
        "model.add(Dense(32, kernel_initializer='normal', activation='relu')) # Hidden 3\n",
        "model.add(Dense(16, kernel_initializer='normal', activation='relu')) # Hidden 4\n",
        "\n",
        "# model.add(Dense(1, kernel_initializer='normal', activation='linear')) \n",
        "model.add(Dense(1)) # Output\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n",
        "                        patience=500, verbose=1, mode='auto', \n",
        "                        restore_best_weights=True)\n",
        "model.fit(x_train,y_train,validation_data=(x_test,y_test),\n",
        "          callbacks=[monitor],verbose=2,epochs=1000, batch_size=64)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "6/6 - 1s - loss: 23132428.0000 - val_loss: 32302354.0000\n",
            "Epoch 2/1000\n",
            "6/6 - 0s - loss: 10140805.0000 - val_loss: 2168934.0000\n",
            "Epoch 3/1000\n",
            "6/6 - 0s - loss: 7252790.5000 - val_loss: 2237021.5000\n",
            "Epoch 4/1000\n",
            "6/6 - 0s - loss: 6683353.5000 - val_loss: 1616562.0000\n",
            "Epoch 5/1000\n",
            "6/6 - 0s - loss: 6594105.0000 - val_loss: 1737978.3750\n",
            "Epoch 6/1000\n",
            "6/6 - 0s - loss: 6446057.0000 - val_loss: 1594978.5000\n",
            "Epoch 7/1000\n",
            "6/6 - 0s - loss: 6337352.0000 - val_loss: 2187611.7500\n",
            "Epoch 8/1000\n",
            "6/6 - 0s - loss: 6198662.5000 - val_loss: 1536985.5000\n",
            "Epoch 9/1000\n",
            "6/6 - 0s - loss: 6098056.5000 - val_loss: 1643255.8750\n",
            "Epoch 10/1000\n",
            "6/6 - 0s - loss: 5611963.5000 - val_loss: 1547589.8750\n",
            "Epoch 11/1000\n",
            "6/6 - 0s - loss: 5170839.5000 - val_loss: 1990572.1250\n",
            "Epoch 12/1000\n",
            "6/6 - 0s - loss: 4631377.0000 - val_loss: 1569049.7500\n",
            "Epoch 13/1000\n",
            "6/6 - 0s - loss: 3818420.5000 - val_loss: 1272206.2500\n",
            "Epoch 14/1000\n",
            "6/6 - 0s - loss: 3033851.2500 - val_loss: 1362379.3750\n",
            "Epoch 15/1000\n",
            "6/6 - 0s - loss: 2236938.0000 - val_loss: 1017041.2500\n",
            "Epoch 16/1000\n",
            "6/6 - 0s - loss: 1728349.3750 - val_loss: 999251.7500\n",
            "Epoch 17/1000\n",
            "6/6 - 0s - loss: 1747429.7500 - val_loss: 1066166.6250\n",
            "Epoch 18/1000\n",
            "6/6 - 0s - loss: 1706640.6250 - val_loss: 1113944.0000\n",
            "Epoch 19/1000\n",
            "6/6 - 0s - loss: 1597939.3750 - val_loss: 1109180.5000\n",
            "Epoch 20/1000\n",
            "6/6 - 0s - loss: 1575808.6250 - val_loss: 1151589.7500\n",
            "Epoch 21/1000\n",
            "6/6 - 0s - loss: 1518704.3750 - val_loss: 1059659.8750\n",
            "Epoch 22/1000\n",
            "6/6 - 0s - loss: 1392815.3750 - val_loss: 1019300.1250\n",
            "Epoch 23/1000\n",
            "6/6 - 0s - loss: 1383068.8750 - val_loss: 988738.2500\n",
            "Epoch 24/1000\n",
            "6/6 - 0s - loss: 1335992.2500 - val_loss: 980788.8125\n",
            "Epoch 25/1000\n",
            "6/6 - 0s - loss: 1262403.0000 - val_loss: 977382.4375\n",
            "Epoch 26/1000\n",
            "6/6 - 0s - loss: 1233005.6250 - val_loss: 944255.6250\n",
            "Epoch 27/1000\n",
            "6/6 - 0s - loss: 1208246.5000 - val_loss: 939541.1875\n",
            "Epoch 28/1000\n",
            "6/6 - 0s - loss: 1144929.7500 - val_loss: 914000.3750\n",
            "Epoch 29/1000\n",
            "6/6 - 0s - loss: 1143174.5000 - val_loss: 957756.1250\n",
            "Epoch 30/1000\n",
            "6/6 - 0s - loss: 1123701.6250 - val_loss: 906538.9375\n",
            "Epoch 31/1000\n",
            "6/6 - 0s - loss: 1109044.5000 - val_loss: 884086.7500\n",
            "Epoch 32/1000\n",
            "6/6 - 0s - loss: 1036233.9375 - val_loss: 868233.1250\n",
            "Epoch 33/1000\n",
            "6/6 - 0s - loss: 977451.3125 - val_loss: 911054.4375\n",
            "Epoch 34/1000\n",
            "6/6 - 0s - loss: 911439.3750 - val_loss: 755680.7500\n",
            "Epoch 35/1000\n",
            "6/6 - 0s - loss: 881081.5625 - val_loss: 822262.4375\n",
            "Epoch 36/1000\n",
            "6/6 - 0s - loss: 799246.0000 - val_loss: 778314.1250\n",
            "Epoch 37/1000\n",
            "6/6 - 0s - loss: 803775.3125 - val_loss: 700208.1875\n",
            "Epoch 38/1000\n",
            "6/6 - 0s - loss: 676986.0625 - val_loss: 599275.1250\n",
            "Epoch 39/1000\n",
            "6/6 - 0s - loss: 669333.0625 - val_loss: 956200.7500\n",
            "Epoch 40/1000\n",
            "6/6 - 0s - loss: 730780.3125 - val_loss: 953797.8750\n",
            "Epoch 41/1000\n",
            "6/6 - 0s - loss: 609996.9375 - val_loss: 805295.5000\n",
            "Epoch 42/1000\n",
            "6/6 - 0s - loss: 598513.1250 - val_loss: 864520.3125\n",
            "Epoch 43/1000\n",
            "6/6 - 0s - loss: 467312.7500 - val_loss: 644637.0625\n",
            "Epoch 44/1000\n",
            "6/6 - 0s - loss: 390716.6250 - val_loss: 587133.7500\n",
            "Epoch 45/1000\n",
            "6/6 - 0s - loss: 316456.6250 - val_loss: 504626.5625\n",
            "Epoch 46/1000\n",
            "6/6 - 0s - loss: 281684.1562 - val_loss: 424176.1250\n",
            "Epoch 47/1000\n",
            "6/6 - 0s - loss: 288014.9688 - val_loss: 399384.7188\n",
            "Epoch 48/1000\n",
            "6/6 - 0s - loss: 338034.1562 - val_loss: 391436.8750\n",
            "Epoch 49/1000\n",
            "6/6 - 0s - loss: 326060.4062 - val_loss: 1815254.5000\n",
            "Epoch 50/1000\n",
            "6/6 - 0s - loss: 2056390.0000 - val_loss: 1812246.6250\n",
            "Epoch 51/1000\n",
            "6/6 - 0s - loss: 912208.9375 - val_loss: 1047986.8125\n",
            "Epoch 52/1000\n",
            "6/6 - 0s - loss: 757309.5000 - val_loss: 1024483.6875\n",
            "Epoch 53/1000\n",
            "6/6 - 0s - loss: 760294.6250 - val_loss: 1177596.8750\n",
            "Epoch 54/1000\n",
            "6/6 - 0s - loss: 699172.7500 - val_loss: 1021201.6250\n",
            "Epoch 55/1000\n",
            "6/6 - 0s - loss: 691659.5625 - val_loss: 1007870.6875\n",
            "Epoch 56/1000\n",
            "6/6 - 0s - loss: 638545.5625 - val_loss: 976729.6875\n",
            "Epoch 57/1000\n",
            "6/6 - 0s - loss: 634069.8125 - val_loss: 966723.3750\n",
            "Epoch 58/1000\n",
            "6/6 - 0s - loss: 590952.3750 - val_loss: 990264.3750\n",
            "Epoch 59/1000\n",
            "6/6 - 0s - loss: 545257.5625 - val_loss: 945373.5000\n",
            "Epoch 60/1000\n",
            "6/6 - 0s - loss: 442314.5938 - val_loss: 911804.9375\n",
            "Epoch 61/1000\n",
            "6/6 - 0s - loss: 428123.5938 - val_loss: 865022.3750\n",
            "Epoch 62/1000\n",
            "6/6 - 0s - loss: 404720.2812 - val_loss: 846606.6250\n",
            "Epoch 63/1000\n",
            "6/6 - 0s - loss: 391229.5625 - val_loss: 912265.1875\n",
            "Epoch 64/1000\n",
            "6/6 - 0s - loss: 330994.4688 - val_loss: 866383.7500\n",
            "Epoch 65/1000\n",
            "6/6 - 0s - loss: 383949.1562 - val_loss: 814756.3750\n",
            "Epoch 66/1000\n",
            "6/6 - 0s - loss: 353028.4062 - val_loss: 717560.2500\n",
            "Epoch 67/1000\n",
            "6/6 - 0s - loss: 311231.7500 - val_loss: 623950.0000\n",
            "Epoch 68/1000\n",
            "6/6 - 0s - loss: 422821.4688 - val_loss: 812681.6875\n",
            "Epoch 69/1000\n",
            "6/6 - 0s - loss: 542733.3750 - val_loss: 1130299.7500\n",
            "Epoch 70/1000\n",
            "6/6 - 0s - loss: 586534.8750 - val_loss: 1058620.3750\n",
            "Epoch 71/1000\n",
            "6/6 - 0s - loss: 570707.6250 - val_loss: 1171524.0000\n",
            "Epoch 72/1000\n",
            "6/6 - 0s - loss: 540218.0625 - val_loss: 1529708.3750\n",
            "Epoch 73/1000\n",
            "6/6 - 0s - loss: 603674.4375 - val_loss: 861741.1250\n",
            "Epoch 74/1000\n",
            "6/6 - 0s - loss: 422081.5000 - val_loss: 658789.9375\n",
            "Epoch 75/1000\n",
            "6/6 - 0s - loss: 380264.8125 - val_loss: 606919.6250\n",
            "Epoch 76/1000\n",
            "6/6 - 0s - loss: 404094.5312 - val_loss: 565628.8750\n",
            "Epoch 77/1000\n",
            "6/6 - 0s - loss: 351853.8125 - val_loss: 517697.6562\n",
            "Epoch 78/1000\n",
            "6/6 - 0s - loss: 316394.8750 - val_loss: 518380.2500\n",
            "Epoch 79/1000\n",
            "6/6 - 0s - loss: 273531.9688 - val_loss: 432169.3750\n",
            "Epoch 80/1000\n",
            "6/6 - 0s - loss: 257644.5938 - val_loss: 509927.4062\n",
            "Epoch 81/1000\n",
            "6/6 - 0s - loss: 266394.2812 - val_loss: 483458.0000\n",
            "Epoch 82/1000\n",
            "6/6 - 0s - loss: 241745.5312 - val_loss: 458889.8438\n",
            "Epoch 83/1000\n",
            "6/6 - 0s - loss: 242711.8750 - val_loss: 457765.9375\n",
            "Epoch 84/1000\n",
            "6/6 - 0s - loss: 217725.0625 - val_loss: 480571.9062\n",
            "Epoch 85/1000\n",
            "6/6 - 0s - loss: 213409.5469 - val_loss: 483245.7812\n",
            "Epoch 86/1000\n",
            "6/6 - 0s - loss: 254813.5469 - val_loss: 455784.2500\n",
            "Epoch 87/1000\n",
            "6/6 - 0s - loss: 239921.8438 - val_loss: 497594.5938\n",
            "Epoch 88/1000\n",
            "6/6 - 0s - loss: 700036.4375 - val_loss: 1085772.0000\n",
            "Epoch 89/1000\n",
            "6/6 - 0s - loss: 805662.1250 - val_loss: 1265601.8750\n",
            "Epoch 90/1000\n",
            "6/6 - 0s - loss: 793646.6875 - val_loss: 1093016.0000\n",
            "Epoch 91/1000\n",
            "6/6 - 0s - loss: 772973.8750 - val_loss: 1111902.1250\n",
            "Epoch 92/1000\n",
            "6/6 - 0s - loss: 766728.8750 - val_loss: 1027009.6875\n",
            "Epoch 93/1000\n",
            "6/6 - 0s - loss: 775697.1250 - val_loss: 1019396.7500\n",
            "Epoch 94/1000\n",
            "6/6 - 0s - loss: 737910.8750 - val_loss: 1191301.8750\n",
            "Epoch 95/1000\n",
            "6/6 - 0s - loss: 705955.0000 - val_loss: 1158680.5000\n",
            "Epoch 96/1000\n",
            "6/6 - 0s - loss: 717096.2500 - val_loss: 1382803.1250\n",
            "Epoch 97/1000\n",
            "6/6 - 0s - loss: 726162.6250 - val_loss: 1015142.1250\n",
            "Epoch 98/1000\n",
            "6/6 - 0s - loss: 664831.3125 - val_loss: 1030309.0000\n",
            "Epoch 99/1000\n",
            "6/6 - 0s - loss: 679592.1250 - val_loss: 1011261.6875\n",
            "Epoch 100/1000\n",
            "6/6 - 0s - loss: 668908.8750 - val_loss: 1020452.9375\n",
            "Epoch 101/1000\n",
            "6/6 - 0s - loss: 650705.0000 - val_loss: 1119034.5000\n",
            "Epoch 102/1000\n",
            "6/6 - 0s - loss: 629230.9375 - val_loss: 1014383.8125\n",
            "Epoch 103/1000\n",
            "6/6 - 0s - loss: 624232.8750 - val_loss: 1035736.6250\n",
            "Epoch 104/1000\n",
            "6/6 - 0s - loss: 616824.7500 - val_loss: 1036912.3125\n",
            "Epoch 105/1000\n",
            "6/6 - 0s - loss: 604430.1875 - val_loss: 1017906.3750\n",
            "Epoch 106/1000\n",
            "6/6 - 0s - loss: 584434.8125 - val_loss: 1017151.0625\n",
            "Epoch 107/1000\n",
            "6/6 - 0s - loss: 570012.3125 - val_loss: 1004951.8750\n",
            "Epoch 108/1000\n",
            "6/6 - 0s - loss: 557332.3750 - val_loss: 1011754.8750\n",
            "Epoch 109/1000\n",
            "6/6 - 0s - loss: 546075.1250 - val_loss: 976445.5625\n",
            "Epoch 110/1000\n",
            "6/6 - 0s - loss: 535491.7500 - val_loss: 999285.8750\n",
            "Epoch 111/1000\n",
            "6/6 - 0s - loss: 521842.0938 - val_loss: 952350.2500\n",
            "Epoch 112/1000\n",
            "6/6 - 0s - loss: 511812.1875 - val_loss: 962684.3750\n",
            "Epoch 113/1000\n",
            "6/6 - 0s - loss: 502545.2500 - val_loss: 885583.2500\n",
            "Epoch 114/1000\n",
            "6/6 - 0s - loss: 483050.3438 - val_loss: 876635.7500\n",
            "Epoch 115/1000\n",
            "6/6 - 0s - loss: 464117.6562 - val_loss: 753342.0625\n",
            "Epoch 116/1000\n",
            "6/6 - 0s - loss: 405376.8125 - val_loss: 668633.0000\n",
            "Epoch 117/1000\n",
            "6/6 - 0s - loss: 338795.2812 - val_loss: 563142.5000\n",
            "Epoch 118/1000\n",
            "6/6 - 0s - loss: 320627.4688 - val_loss: 548782.4375\n",
            "Epoch 119/1000\n",
            "6/6 - 0s - loss: 306530.3438 - val_loss: 492588.6875\n",
            "Epoch 120/1000\n",
            "6/6 - 0s - loss: 302145.2188 - val_loss: 570177.7500\n",
            "Epoch 121/1000\n",
            "6/6 - 0s - loss: 283988.7812 - val_loss: 443534.8750\n",
            "Epoch 122/1000\n",
            "6/6 - 0s - loss: 335403.1250 - val_loss: 478454.5625\n",
            "Epoch 123/1000\n",
            "6/6 - 0s - loss: 304052.3750 - val_loss: 543242.3750\n",
            "Epoch 124/1000\n",
            "6/6 - 0s - loss: 278845.4688 - val_loss: 387423.5625\n",
            "Epoch 125/1000\n",
            "6/6 - 0s - loss: 243388.2969 - val_loss: 381471.1250\n",
            "Epoch 126/1000\n",
            "6/6 - 0s - loss: 381424.0938 - val_loss: 403895.3750\n",
            "Epoch 127/1000\n",
            "6/6 - 0s - loss: 292322.6875 - val_loss: 409313.9062\n",
            "Epoch 128/1000\n",
            "6/6 - 0s - loss: 418365.4688 - val_loss: 365732.3125\n",
            "Epoch 129/1000\n",
            "6/6 - 0s - loss: 311299.2812 - val_loss: 352551.6250\n",
            "Epoch 130/1000\n",
            "6/6 - 0s - loss: 315497.8750 - val_loss: 307662.2500\n",
            "Epoch 131/1000\n",
            "6/6 - 0s - loss: 299760.4062 - val_loss: 361356.9688\n",
            "Epoch 132/1000\n",
            "6/6 - 0s - loss: 278082.2188 - val_loss: 429352.5938\n",
            "Epoch 133/1000\n",
            "6/6 - 0s - loss: 253447.9531 - val_loss: 293401.5625\n",
            "Epoch 134/1000\n",
            "6/6 - 0s - loss: 206072.7500 - val_loss: 411219.1250\n",
            "Epoch 135/1000\n",
            "6/6 - 0s - loss: 433224.5312 - val_loss: 448161.9375\n",
            "Epoch 136/1000\n",
            "6/6 - 0s - loss: 413349.4688 - val_loss: 476688.1875\n",
            "Epoch 137/1000\n",
            "6/6 - 0s - loss: 389409.4375 - val_loss: 470624.3750\n",
            "Epoch 138/1000\n",
            "6/6 - 0s - loss: 325990.5000 - val_loss: 435874.2812\n",
            "Epoch 139/1000\n",
            "6/6 - 0s - loss: 308709.9062 - val_loss: 355324.9375\n",
            "Epoch 140/1000\n",
            "6/6 - 0s - loss: 260887.3125 - val_loss: 300944.2188\n",
            "Epoch 141/1000\n",
            "6/6 - 0s - loss: 239621.2031 - val_loss: 308188.9375\n",
            "Epoch 142/1000\n",
            "6/6 - 0s - loss: 229227.7344 - val_loss: 271369.1875\n",
            "Epoch 143/1000\n",
            "6/6 - 0s - loss: 199946.2500 - val_loss: 275242.3125\n",
            "Epoch 144/1000\n",
            "6/6 - 0s - loss: 206624.0781 - val_loss: 300710.1250\n",
            "Epoch 145/1000\n",
            "6/6 - 0s - loss: 194851.9844 - val_loss: 297016.0312\n",
            "Epoch 146/1000\n",
            "6/6 - 0s - loss: 208325.1250 - val_loss: 463873.1250\n",
            "Epoch 147/1000\n",
            "6/6 - 0s - loss: 302301.0312 - val_loss: 399394.9688\n",
            "Epoch 148/1000\n",
            "6/6 - 0s - loss: 268848.0312 - val_loss: 365225.2188\n",
            "Epoch 149/1000\n",
            "6/6 - 0s - loss: 293258.6562 - val_loss: 442821.7500\n",
            "Epoch 150/1000\n",
            "6/6 - 0s - loss: 228670.9531 - val_loss: 351766.5938\n",
            "Epoch 151/1000\n",
            "6/6 - 0s - loss: 194770.0781 - val_loss: 308295.5625\n",
            "Epoch 152/1000\n",
            "6/6 - 0s - loss: 321383.5000 - val_loss: 499320.8750\n",
            "Epoch 153/1000\n",
            "6/6 - 0s - loss: 311652.7188 - val_loss: 364202.3750\n",
            "Epoch 154/1000\n",
            "6/6 - 0s - loss: 316257.3125 - val_loss: 304533.3750\n",
            "Epoch 155/1000\n",
            "6/6 - 0s - loss: 214429.0000 - val_loss: 370573.7500\n",
            "Epoch 156/1000\n",
            "6/6 - 0s - loss: 202895.5625 - val_loss: 265837.2188\n",
            "Epoch 157/1000\n",
            "6/6 - 0s - loss: 229775.2500 - val_loss: 329655.5625\n",
            "Epoch 158/1000\n",
            "6/6 - 0s - loss: 284784.4375 - val_loss: 292855.9375\n",
            "Epoch 159/1000\n",
            "6/6 - 0s - loss: 162848.4219 - val_loss: 319184.3750\n",
            "Epoch 160/1000\n",
            "6/6 - 0s - loss: 189218.4375 - val_loss: 263725.9062\n",
            "Epoch 161/1000\n",
            "6/6 - 0s - loss: 151202.7344 - val_loss: 265306.0938\n",
            "Epoch 162/1000\n",
            "6/6 - 0s - loss: 135812.3594 - val_loss: 242259.0312\n",
            "Epoch 163/1000\n",
            "6/6 - 0s - loss: 148477.0312 - val_loss: 300089.7188\n",
            "Epoch 164/1000\n",
            "6/6 - 0s - loss: 190897.5312 - val_loss: 248017.3125\n",
            "Epoch 165/1000\n",
            "6/6 - 0s - loss: 150364.8125 - val_loss: 251704.8438\n",
            "Epoch 166/1000\n",
            "6/6 - 0s - loss: 132206.7344 - val_loss: 367176.8438\n",
            "Epoch 167/1000\n",
            "6/6 - 0s - loss: 139949.5625 - val_loss: 335344.1562\n",
            "Epoch 168/1000\n",
            "6/6 - 0s - loss: 226716.2188 - val_loss: 230055.4375\n",
            "Epoch 169/1000\n",
            "6/6 - 0s - loss: 164597.4531 - val_loss: 253160.4688\n",
            "Epoch 170/1000\n",
            "6/6 - 0s - loss: 158903.2812 - val_loss: 325024.2500\n",
            "Epoch 171/1000\n",
            "6/6 - 0s - loss: 163239.0938 - val_loss: 335353.5938\n",
            "Epoch 172/1000\n",
            "6/6 - 0s - loss: 219683.6250 - val_loss: 249977.6562\n",
            "Epoch 173/1000\n",
            "6/6 - 0s - loss: 191943.3906 - val_loss: 232096.0000\n",
            "Epoch 174/1000\n",
            "6/6 - 0s - loss: 161469.6250 - val_loss: 231736.9375\n",
            "Epoch 175/1000\n",
            "6/6 - 0s - loss: 103918.1484 - val_loss: 765942.3750\n",
            "Epoch 176/1000\n",
            "6/6 - 0s - loss: 208168.7344 - val_loss: 388724.4062\n",
            "Epoch 177/1000\n",
            "6/6 - 0s - loss: 213319.9844 - val_loss: 266451.9375\n",
            "Epoch 178/1000\n",
            "6/6 - 0s - loss: 277574.8750 - val_loss: 352467.1250\n",
            "Epoch 179/1000\n",
            "6/6 - 0s - loss: 346565.0312 - val_loss: 355739.3750\n",
            "Epoch 180/1000\n",
            "6/6 - 0s - loss: 297346.6250 - val_loss: 331686.5938\n",
            "Epoch 181/1000\n",
            "6/6 - 0s - loss: 178005.5781 - val_loss: 491854.0000\n",
            "Epoch 182/1000\n",
            "6/6 - 0s - loss: 162433.5469 - val_loss: 252250.4375\n",
            "Epoch 183/1000\n",
            "6/6 - 0s - loss: 161958.5156 - val_loss: 256860.2500\n",
            "Epoch 184/1000\n",
            "6/6 - 0s - loss: 137707.0781 - val_loss: 248556.8281\n",
            "Epoch 185/1000\n",
            "6/6 - 0s - loss: 147700.7500 - val_loss: 241010.0000\n",
            "Epoch 186/1000\n",
            "6/6 - 0s - loss: 94396.8594 - val_loss: 385903.2500\n",
            "Epoch 187/1000\n",
            "6/6 - 0s - loss: 95238.8125 - val_loss: 195629.8750\n",
            "Epoch 188/1000\n",
            "6/6 - 0s - loss: 106453.2891 - val_loss: 257550.6562\n",
            "Epoch 189/1000\n",
            "6/6 - 0s - loss: 274212.4062 - val_loss: 350709.5625\n",
            "Epoch 190/1000\n",
            "6/6 - 0s - loss: 321056.2188 - val_loss: 324822.3438\n",
            "Epoch 191/1000\n",
            "6/6 - 0s - loss: 244302.3594 - val_loss: 250883.1094\n",
            "Epoch 192/1000\n",
            "6/6 - 0s - loss: 131868.6562 - val_loss: 291663.4062\n",
            "Epoch 193/1000\n",
            "6/6 - 0s - loss: 106549.3125 - val_loss: 232246.2344\n",
            "Epoch 194/1000\n",
            "6/6 - 0s - loss: 115153.0469 - val_loss: 197584.1875\n",
            "Epoch 195/1000\n",
            "6/6 - 0s - loss: 112287.8203 - val_loss: 196122.7812\n",
            "Epoch 196/1000\n",
            "6/6 - 0s - loss: 109775.4766 - val_loss: 181504.6250\n",
            "Epoch 197/1000\n",
            "6/6 - 0s - loss: 164217.0156 - val_loss: 382249.0000\n",
            "Epoch 198/1000\n",
            "6/6 - 0s - loss: 700272.5625 - val_loss: 715804.8750\n",
            "Epoch 199/1000\n",
            "6/6 - 0s - loss: 558847.5625 - val_loss: 762247.4375\n",
            "Epoch 200/1000\n",
            "6/6 - 0s - loss: 583114.0000 - val_loss: 845828.8125\n",
            "Epoch 201/1000\n",
            "6/6 - 0s - loss: 547915.0000 - val_loss: 861906.7500\n",
            "Epoch 202/1000\n",
            "6/6 - 0s - loss: 539069.3125 - val_loss: 821702.1250\n",
            "Epoch 203/1000\n",
            "6/6 - 0s - loss: 519678.9062 - val_loss: 760649.3750\n",
            "Epoch 204/1000\n",
            "6/6 - 0s - loss: 515182.8750 - val_loss: 702043.2500\n",
            "Epoch 205/1000\n",
            "6/6 - 0s - loss: 482009.2812 - val_loss: 677997.3125\n",
            "Epoch 206/1000\n",
            "6/6 - 0s - loss: 475625.2812 - val_loss: 535517.0000\n",
            "Epoch 207/1000\n",
            "6/6 - 0s - loss: 418888.3125 - val_loss: 454920.5625\n",
            "Epoch 208/1000\n",
            "6/6 - 0s - loss: 336727.3125 - val_loss: 446487.1562\n",
            "Epoch 209/1000\n",
            "6/6 - 0s - loss: 295307.0938 - val_loss: 288825.0625\n",
            "Epoch 210/1000\n",
            "6/6 - 0s - loss: 233540.5781 - val_loss: 336149.8125\n",
            "Epoch 211/1000\n",
            "6/6 - 0s - loss: 192662.6406 - val_loss: 316606.9375\n",
            "Epoch 212/1000\n",
            "6/6 - 0s - loss: 192920.1875 - val_loss: 297328.8750\n",
            "Epoch 213/1000\n",
            "6/6 - 0s - loss: 160455.4062 - val_loss: 205244.8750\n",
            "Epoch 214/1000\n",
            "6/6 - 0s - loss: 145949.8125 - val_loss: 212554.0156\n",
            "Epoch 215/1000\n",
            "6/6 - 0s - loss: 130260.1719 - val_loss: 211709.4688\n",
            "Epoch 216/1000\n",
            "6/6 - 0s - loss: 114433.9141 - val_loss: 228781.6719\n",
            "Epoch 217/1000\n",
            "6/6 - 0s - loss: 97868.3359 - val_loss: 303263.1562\n",
            "Epoch 218/1000\n",
            "6/6 - 0s - loss: 129329.0781 - val_loss: 188395.6562\n",
            "Epoch 219/1000\n",
            "6/6 - 0s - loss: 99202.6875 - val_loss: 322095.3438\n",
            "Epoch 220/1000\n",
            "6/6 - 0s - loss: 139692.9375 - val_loss: 270766.7500\n",
            "Epoch 221/1000\n",
            "6/6 - 0s - loss: 116816.9062 - val_loss: 232941.8438\n",
            "Epoch 222/1000\n",
            "6/6 - 0s - loss: 254892.5938 - val_loss: 348444.8750\n",
            "Epoch 223/1000\n",
            "6/6 - 0s - loss: 336425.7812 - val_loss: 286919.5000\n",
            "Epoch 224/1000\n",
            "6/6 - 0s - loss: 283443.2188 - val_loss: 249558.2500\n",
            "Epoch 225/1000\n",
            "6/6 - 0s - loss: 211191.3281 - val_loss: 184760.3125\n",
            "Epoch 226/1000\n",
            "6/6 - 0s - loss: 158160.3750 - val_loss: 213255.5938\n",
            "Epoch 227/1000\n",
            "6/6 - 0s - loss: 145716.4688 - val_loss: 158339.6719\n",
            "Epoch 228/1000\n",
            "6/6 - 0s - loss: 145927.4688 - val_loss: 170957.1562\n",
            "Epoch 229/1000\n",
            "6/6 - 0s - loss: 199404.0469 - val_loss: 238335.5938\n",
            "Epoch 230/1000\n",
            "6/6 - 0s - loss: 184394.3594 - val_loss: 196927.1875\n",
            "Epoch 231/1000\n",
            "6/6 - 0s - loss: 73122.4141 - val_loss: 191910.4844\n",
            "Epoch 232/1000\n",
            "6/6 - 0s - loss: 95511.3359 - val_loss: 147593.0156\n",
            "Epoch 233/1000\n",
            "6/6 - 0s - loss: 107045.6250 - val_loss: 189260.7812\n",
            "Epoch 234/1000\n",
            "6/6 - 0s - loss: 85493.2500 - val_loss: 153097.3125\n",
            "Epoch 235/1000\n",
            "6/6 - 0s - loss: 73273.9922 - val_loss: 153340.5000\n",
            "Epoch 236/1000\n",
            "6/6 - 0s - loss: 51097.7852 - val_loss: 116011.2578\n",
            "Epoch 237/1000\n",
            "6/6 - 0s - loss: 100153.0625 - val_loss: 218359.1719\n",
            "Epoch 238/1000\n",
            "6/6 - 0s - loss: 228876.1562 - val_loss: 206929.5312\n",
            "Epoch 239/1000\n",
            "6/6 - 0s - loss: 163753.8750 - val_loss: 198247.3125\n",
            "Epoch 240/1000\n",
            "6/6 - 0s - loss: 80618.0391 - val_loss: 193763.6094\n",
            "Epoch 241/1000\n",
            "6/6 - 0s - loss: 132533.8750 - val_loss: 112447.9297\n",
            "Epoch 242/1000\n",
            "6/6 - 0s - loss: 82607.5781 - val_loss: 152256.1562\n",
            "Epoch 243/1000\n",
            "6/6 - 0s - loss: 73986.6953 - val_loss: 167675.6875\n",
            "Epoch 244/1000\n",
            "6/6 - 0s - loss: 133160.6875 - val_loss: 200652.2500\n",
            "Epoch 245/1000\n",
            "6/6 - 0s - loss: 159487.2500 - val_loss: 155329.8906\n",
            "Epoch 246/1000\n",
            "6/6 - 0s - loss: 221350.9219 - val_loss: 451340.0938\n",
            "Epoch 247/1000\n",
            "6/6 - 0s - loss: 428787.6562 - val_loss: 556922.1250\n",
            "Epoch 248/1000\n",
            "6/6 - 0s - loss: 445996.6875 - val_loss: 565564.3125\n",
            "Epoch 249/1000\n",
            "6/6 - 0s - loss: 422661.1562 - val_loss: 516632.5000\n",
            "Epoch 250/1000\n",
            "6/6 - 0s - loss: 360648.5938 - val_loss: 516912.4375\n",
            "Epoch 251/1000\n",
            "6/6 - 0s - loss: 321398.8125 - val_loss: 395408.6562\n",
            "Epoch 252/1000\n",
            "6/6 - 0s - loss: 258004.6562 - val_loss: 313805.8438\n",
            "Epoch 253/1000\n",
            "6/6 - 0s - loss: 211987.9062 - val_loss: 322354.6875\n",
            "Epoch 254/1000\n",
            "6/6 - 0s - loss: 208273.1094 - val_loss: 299871.0625\n",
            "Epoch 255/1000\n",
            "6/6 - 0s - loss: 193540.7500 - val_loss: 310449.3125\n",
            "Epoch 256/1000\n",
            "6/6 - 0s - loss: 205653.1250 - val_loss: 281773.3750\n",
            "Epoch 257/1000\n",
            "6/6 - 0s - loss: 284073.6250 - val_loss: 299625.3125\n",
            "Epoch 258/1000\n",
            "6/6 - 0s - loss: 244011.6562 - val_loss: 296993.6250\n",
            "Epoch 259/1000\n",
            "6/6 - 0s - loss: 208326.4219 - val_loss: 205802.0000\n",
            "Epoch 260/1000\n",
            "6/6 - 0s - loss: 119402.7891 - val_loss: 182866.7188\n",
            "Epoch 261/1000\n",
            "6/6 - 0s - loss: 128921.2266 - val_loss: 142090.8906\n",
            "Epoch 262/1000\n",
            "6/6 - 0s - loss: 98792.3906 - val_loss: 149315.8125\n",
            "Epoch 263/1000\n",
            "6/6 - 0s - loss: 123630.7188 - val_loss: 126147.3281\n",
            "Epoch 264/1000\n",
            "6/6 - 0s - loss: 61728.6250 - val_loss: 122204.1328\n",
            "Epoch 265/1000\n",
            "6/6 - 0s - loss: 90532.9375 - val_loss: 195940.2969\n",
            "Epoch 266/1000\n",
            "6/6 - 0s - loss: 219135.3438 - val_loss: 238155.3438\n",
            "Epoch 267/1000\n",
            "6/6 - 0s - loss: 217953.0781 - val_loss: 224264.3750\n",
            "Epoch 268/1000\n",
            "6/6 - 0s - loss: 109366.3750 - val_loss: 201280.8438\n",
            "Epoch 269/1000\n",
            "6/6 - 0s - loss: 72760.9219 - val_loss: 129329.3750\n",
            "Epoch 270/1000\n",
            "6/6 - 0s - loss: 83026.6875 - val_loss: 122730.1094\n",
            "Epoch 271/1000\n",
            "6/6 - 0s - loss: 71541.8516 - val_loss: 141280.9375\n",
            "Epoch 272/1000\n",
            "6/6 - 0s - loss: 86500.3281 - val_loss: 126628.4141\n",
            "Epoch 273/1000\n",
            "6/6 - 0s - loss: 65019.2227 - val_loss: 123454.6797\n",
            "Epoch 274/1000\n",
            "6/6 - 0s - loss: 245321.5781 - val_loss: 436065.7500\n",
            "Epoch 275/1000\n",
            "6/6 - 0s - loss: 475467.2500 - val_loss: 584758.1250\n",
            "Epoch 276/1000\n",
            "6/6 - 0s - loss: 495356.3125 - val_loss: 481840.3750\n",
            "Epoch 277/1000\n",
            "6/6 - 0s - loss: 494366.3750 - val_loss: 447881.5625\n",
            "Epoch 278/1000\n",
            "6/6 - 0s - loss: 414049.8438 - val_loss: 438290.0312\n",
            "Epoch 279/1000\n",
            "6/6 - 0s - loss: 381989.6250 - val_loss: 378749.5312\n",
            "Epoch 280/1000\n",
            "6/6 - 0s - loss: 266383.3750 - val_loss: 221195.0469\n",
            "Epoch 281/1000\n",
            "6/6 - 0s - loss: 243811.5000 - val_loss: 196302.6406\n",
            "Epoch 282/1000\n",
            "6/6 - 0s - loss: 132669.6094 - val_loss: 256571.9062\n",
            "Epoch 283/1000\n",
            "6/6 - 0s - loss: 157749.2812 - val_loss: 167860.8750\n",
            "Epoch 284/1000\n",
            "6/6 - 0s - loss: 118517.9141 - val_loss: 164606.5938\n",
            "Epoch 285/1000\n",
            "6/6 - 0s - loss: 102498.7422 - val_loss: 154530.5938\n",
            "Epoch 286/1000\n",
            "6/6 - 0s - loss: 73385.4766 - val_loss: 152618.3281\n",
            "Epoch 287/1000\n",
            "6/6 - 0s - loss: 56998.1406 - val_loss: 216810.5938\n",
            "Epoch 288/1000\n",
            "6/6 - 0s - loss: 172844.7500 - val_loss: 190579.5156\n",
            "Epoch 289/1000\n",
            "6/6 - 0s - loss: 173793.0000 - val_loss: 208385.3438\n",
            "Epoch 290/1000\n",
            "6/6 - 0s - loss: 151067.2188 - val_loss: 175226.5781\n",
            "Epoch 291/1000\n",
            "6/6 - 0s - loss: 80649.3203 - val_loss: 116427.4844\n",
            "Epoch 292/1000\n",
            "6/6 - 0s - loss: 96591.7266 - val_loss: 140617.4062\n",
            "Epoch 293/1000\n",
            "6/6 - 0s - loss: 47708.1172 - val_loss: 112219.3984\n",
            "Epoch 294/1000\n",
            "6/6 - 0s - loss: 66962.2969 - val_loss: 136384.2812\n",
            "Epoch 295/1000\n",
            "6/6 - 0s - loss: 50639.9492 - val_loss: 98859.1719\n",
            "Epoch 296/1000\n",
            "6/6 - 0s - loss: 42250.6641 - val_loss: 136812.6250\n",
            "Epoch 297/1000\n",
            "6/6 - 0s - loss: 35795.8516 - val_loss: 97074.7656\n",
            "Epoch 298/1000\n",
            "6/6 - 0s - loss: 34650.8047 - val_loss: 106077.2344\n",
            "Epoch 299/1000\n",
            "6/6 - 0s - loss: 29743.6211 - val_loss: 88435.0781\n",
            "Epoch 300/1000\n",
            "6/6 - 0s - loss: 32024.7305 - val_loss: 136307.4531\n",
            "Epoch 301/1000\n",
            "6/6 - 0s - loss: 26663.8926 - val_loss: 84657.2109\n",
            "Epoch 302/1000\n",
            "6/6 - 0s - loss: 29797.4043 - val_loss: 92114.6719\n",
            "Epoch 303/1000\n",
            "6/6 - 0s - loss: 30251.1465 - val_loss: 90368.7188\n",
            "Epoch 304/1000\n",
            "6/6 - 0s - loss: 24934.9570 - val_loss: 114818.7344\n",
            "Epoch 305/1000\n",
            "6/6 - 0s - loss: 25487.4434 - val_loss: 121432.9375\n",
            "Epoch 306/1000\n",
            "6/6 - 0s - loss: 28317.5469 - val_loss: 91124.0469\n",
            "Epoch 307/1000\n",
            "6/6 - 0s - loss: 37279.3398 - val_loss: 85919.3594\n",
            "Epoch 308/1000\n",
            "6/6 - 0s - loss: 38962.5234 - val_loss: 104823.7812\n",
            "Epoch 309/1000\n",
            "6/6 - 0s - loss: 45901.1758 - val_loss: 111103.5781\n",
            "Epoch 310/1000\n",
            "6/6 - 0s - loss: 45526.7148 - val_loss: 195041.6719\n",
            "Epoch 311/1000\n",
            "6/6 - 0s - loss: 33183.8281 - val_loss: 91707.7578\n",
            "Epoch 312/1000\n",
            "6/6 - 0s - loss: 32366.6777 - val_loss: 141498.0625\n",
            "Epoch 313/1000\n",
            "6/6 - 0s - loss: 26606.4336 - val_loss: 84873.2734\n",
            "Epoch 314/1000\n",
            "6/6 - 0s - loss: 23136.1016 - val_loss: 81994.6875\n",
            "Epoch 315/1000\n",
            "6/6 - 0s - loss: 24199.5957 - val_loss: 103110.0625\n",
            "Epoch 316/1000\n",
            "6/6 - 0s - loss: 22908.3457 - val_loss: 84329.8516\n",
            "Epoch 317/1000\n",
            "6/6 - 0s - loss: 25129.6719 - val_loss: 77470.2734\n",
            "Epoch 318/1000\n",
            "6/6 - 0s - loss: 23337.0742 - val_loss: 79009.4297\n",
            "Epoch 319/1000\n",
            "6/6 - 0s - loss: 23325.2051 - val_loss: 94161.1797\n",
            "Epoch 320/1000\n",
            "6/6 - 0s - loss: 21181.3086 - val_loss: 86748.7500\n",
            "Epoch 321/1000\n",
            "6/6 - 0s - loss: 20851.7988 - val_loss: 83635.5312\n",
            "Epoch 322/1000\n",
            "6/6 - 0s - loss: 23308.0742 - val_loss: 81575.0781\n",
            "Epoch 323/1000\n",
            "6/6 - 0s - loss: 25287.8652 - val_loss: 93324.7344\n",
            "Epoch 324/1000\n",
            "6/6 - 0s - loss: 20209.3164 - val_loss: 89351.8125\n",
            "Epoch 325/1000\n",
            "6/6 - 0s - loss: 18726.0918 - val_loss: 88198.6719\n",
            "Epoch 326/1000\n",
            "6/6 - 0s - loss: 20569.5352 - val_loss: 73656.7188\n",
            "Epoch 327/1000\n",
            "6/6 - 0s - loss: 21213.9707 - val_loss: 70865.5391\n",
            "Epoch 328/1000\n",
            "6/6 - 0s - loss: 18273.3867 - val_loss: 72311.7812\n",
            "Epoch 329/1000\n",
            "6/6 - 0s - loss: 17086.5391 - val_loss: 80260.1406\n",
            "Epoch 330/1000\n",
            "6/6 - 0s - loss: 16735.4355 - val_loss: 70860.3906\n",
            "Epoch 331/1000\n",
            "6/6 - 0s - loss: 18191.6055 - val_loss: 77848.3125\n",
            "Epoch 332/1000\n",
            "6/6 - 0s - loss: 17876.5371 - val_loss: 71621.8906\n",
            "Epoch 333/1000\n",
            "6/6 - 0s - loss: 19161.4473 - val_loss: 72440.2344\n",
            "Epoch 334/1000\n",
            "6/6 - 0s - loss: 31397.2676 - val_loss: 91449.0469\n",
            "Epoch 335/1000\n",
            "6/6 - 0s - loss: 25697.0449 - val_loss: 83224.0000\n",
            "Epoch 336/1000\n",
            "6/6 - 0s - loss: 21672.5898 - val_loss: 83867.6250\n",
            "Epoch 337/1000\n",
            "6/6 - 0s - loss: 22845.8535 - val_loss: 101080.8047\n",
            "Epoch 338/1000\n",
            "6/6 - 0s - loss: 20884.2461 - val_loss: 73408.1094\n",
            "Epoch 339/1000\n",
            "6/6 - 0s - loss: 18987.5293 - val_loss: 68572.5469\n",
            "Epoch 340/1000\n",
            "6/6 - 0s - loss: 104982.4688 - val_loss: 399245.7812\n",
            "Epoch 341/1000\n",
            "6/6 - 0s - loss: 455922.6875 - val_loss: 526706.1250\n",
            "Epoch 342/1000\n",
            "6/6 - 0s - loss: 517254.2500 - val_loss: 810025.9375\n",
            "Epoch 343/1000\n",
            "6/6 - 0s - loss: 534451.3125 - val_loss: 618325.8125\n",
            "Epoch 344/1000\n",
            "6/6 - 0s - loss: 470426.4688 - val_loss: 548850.8750\n",
            "Epoch 345/1000\n",
            "6/6 - 0s - loss: 373031.5625 - val_loss: 497529.9375\n",
            "Epoch 346/1000\n",
            "6/6 - 0s - loss: 336073.6250 - val_loss: 384512.3750\n",
            "Epoch 347/1000\n",
            "6/6 - 0s - loss: 232988.1094 - val_loss: 324302.4375\n",
            "Epoch 348/1000\n",
            "6/6 - 0s - loss: 202458.2031 - val_loss: 357157.5625\n",
            "Epoch 349/1000\n",
            "6/6 - 0s - loss: 179775.9375 - val_loss: 322442.7500\n",
            "Epoch 350/1000\n",
            "6/6 - 0s - loss: 172664.8438 - val_loss: 300030.9688\n",
            "Epoch 351/1000\n",
            "6/6 - 0s - loss: 160355.4844 - val_loss: 352790.8125\n",
            "Epoch 352/1000\n",
            "6/6 - 0s - loss: 156850.6406 - val_loss: 320697.1250\n",
            "Epoch 353/1000\n",
            "6/6 - 0s - loss: 159747.2812 - val_loss: 289085.0938\n",
            "Epoch 354/1000\n",
            "6/6 - 0s - loss: 187741.0469 - val_loss: 266315.7812\n",
            "Epoch 355/1000\n",
            "6/6 - 0s - loss: 192119.0625 - val_loss: 311350.4062\n",
            "Epoch 356/1000\n",
            "6/6 - 0s - loss: 142694.8594 - val_loss: 397043.1875\n",
            "Epoch 357/1000\n",
            "6/6 - 0s - loss: 134047.6406 - val_loss: 253047.5781\n",
            "Epoch 358/1000\n",
            "6/6 - 0s - loss: 124195.7188 - val_loss: 254672.6562\n",
            "Epoch 359/1000\n",
            "6/6 - 0s - loss: 118138.0547 - val_loss: 230152.4688\n",
            "Epoch 360/1000\n",
            "6/6 - 0s - loss: 112392.8750 - val_loss: 247331.2812\n",
            "Epoch 361/1000\n",
            "6/6 - 0s - loss: 109252.4375 - val_loss: 222068.7344\n",
            "Epoch 362/1000\n",
            "6/6 - 0s - loss: 98465.2422 - val_loss: 233762.2500\n",
            "Epoch 363/1000\n",
            "6/6 - 0s - loss: 92043.8125 - val_loss: 229937.2188\n",
            "Epoch 364/1000\n",
            "6/6 - 0s - loss: 86142.8125 - val_loss: 273905.4062\n",
            "Epoch 365/1000\n",
            "6/6 - 0s - loss: 173921.5781 - val_loss: 213078.5312\n",
            "Epoch 366/1000\n",
            "6/6 - 0s - loss: 156354.8594 - val_loss: 248731.3750\n",
            "Epoch 367/1000\n",
            "6/6 - 0s - loss: 129902.0469 - val_loss: 185608.3594\n",
            "Epoch 368/1000\n",
            "6/6 - 0s - loss: 328394.2188 - val_loss: 327990.2500\n",
            "Epoch 369/1000\n",
            "6/6 - 0s - loss: 334079.5000 - val_loss: 252842.1250\n",
            "Epoch 370/1000\n",
            "6/6 - 0s - loss: 256257.4062 - val_loss: 202789.8125\n",
            "Epoch 371/1000\n",
            "6/6 - 0s - loss: 136463.2812 - val_loss: 199011.8750\n",
            "Epoch 372/1000\n",
            "6/6 - 0s - loss: 111340.7344 - val_loss: 242782.7969\n",
            "Epoch 373/1000\n",
            "6/6 - 0s - loss: 104618.1406 - val_loss: 162428.5625\n",
            "Epoch 374/1000\n",
            "6/6 - 0s - loss: 80000.9062 - val_loss: 169577.0312\n",
            "Epoch 375/1000\n",
            "6/6 - 0s - loss: 62009.8984 - val_loss: 166051.3438\n",
            "Epoch 376/1000\n",
            "6/6 - 0s - loss: 53523.5742 - val_loss: 134070.7500\n",
            "Epoch 377/1000\n",
            "6/6 - 0s - loss: 45872.3164 - val_loss: 186013.9531\n",
            "Epoch 378/1000\n",
            "6/6 - 0s - loss: 40091.7422 - val_loss: 117732.5469\n",
            "Epoch 379/1000\n",
            "6/6 - 0s - loss: 42821.4766 - val_loss: 148382.9062\n",
            "Epoch 380/1000\n",
            "6/6 - 0s - loss: 42945.6445 - val_loss: 112854.2812\n",
            "Epoch 381/1000\n",
            "6/6 - 0s - loss: 41599.8984 - val_loss: 188990.7344\n",
            "Epoch 382/1000\n",
            "6/6 - 0s - loss: 41999.5625 - val_loss: 106559.1250\n",
            "Epoch 383/1000\n",
            "6/6 - 0s - loss: 50024.0352 - val_loss: 159043.5938\n",
            "Epoch 384/1000\n",
            "6/6 - 0s - loss: 33903.6758 - val_loss: 98499.2344\n",
            "Epoch 385/1000\n",
            "6/6 - 0s - loss: 45051.6641 - val_loss: 168413.4375\n",
            "Epoch 386/1000\n",
            "6/6 - 0s - loss: 38629.8633 - val_loss: 93625.7188\n",
            "Epoch 387/1000\n",
            "6/6 - 0s - loss: 25095.3652 - val_loss: 102201.4453\n",
            "Epoch 388/1000\n",
            "6/6 - 0s - loss: 25666.2969 - val_loss: 110510.2578\n",
            "Epoch 389/1000\n",
            "6/6 - 0s - loss: 33654.0117 - val_loss: 86630.5781\n",
            "Epoch 390/1000\n",
            "6/6 - 0s - loss: 27064.3750 - val_loss: 80489.9688\n",
            "Epoch 391/1000\n",
            "6/6 - 0s - loss: 46087.4453 - val_loss: 95767.0938\n",
            "Epoch 392/1000\n",
            "6/6 - 0s - loss: 39313.3203 - val_loss: 78205.4844\n",
            "Epoch 393/1000\n",
            "6/6 - 0s - loss: 37039.8320 - val_loss: 81690.5781\n",
            "Epoch 394/1000\n",
            "6/6 - 0s - loss: 44654.8867 - val_loss: 95203.7578\n",
            "Epoch 395/1000\n",
            "6/6 - 0s - loss: 47512.7969 - val_loss: 90218.1562\n",
            "Epoch 396/1000\n",
            "6/6 - 0s - loss: 56841.1445 - val_loss: 73215.3750\n",
            "Epoch 397/1000\n",
            "6/6 - 0s - loss: 35255.8594 - val_loss: 86389.0859\n",
            "Epoch 398/1000\n",
            "6/6 - 0s - loss: 36418.1914 - val_loss: 73642.5312\n",
            "Epoch 399/1000\n",
            "6/6 - 0s - loss: 33635.2188 - val_loss: 89355.8594\n",
            "Epoch 400/1000\n",
            "6/6 - 0s - loss: 28232.3984 - val_loss: 85166.2344\n",
            "Epoch 401/1000\n",
            "6/6 - 0s - loss: 24882.0625 - val_loss: 127535.5469\n",
            "Epoch 402/1000\n",
            "6/6 - 0s - loss: 25962.2949 - val_loss: 73297.7969\n",
            "Epoch 403/1000\n",
            "6/6 - 0s - loss: 24563.4980 - val_loss: 74109.1719\n",
            "Epoch 404/1000\n",
            "6/6 - 0s - loss: 30387.3047 - val_loss: 115395.4844\n",
            "Epoch 405/1000\n",
            "6/6 - 0s - loss: 32764.2598 - val_loss: 101436.3828\n",
            "Epoch 406/1000\n",
            "6/6 - 0s - loss: 63545.5859 - val_loss: 110099.4141\n",
            "Epoch 407/1000\n",
            "6/6 - 0s - loss: 290698.1562 - val_loss: 464066.6875\n",
            "Epoch 408/1000\n",
            "6/6 - 0s - loss: 541915.3125 - val_loss: 633350.3750\n",
            "Epoch 409/1000\n",
            "6/6 - 0s - loss: 615126.1250 - val_loss: 715109.5000\n",
            "Epoch 410/1000\n",
            "6/6 - 0s - loss: 572925.3750 - val_loss: 776106.8750\n",
            "Epoch 411/1000\n",
            "6/6 - 0s - loss: 581719.9375 - val_loss: 739853.2500\n",
            "Epoch 412/1000\n",
            "6/6 - 0s - loss: 532871.0625 - val_loss: 655698.0000\n",
            "Epoch 413/1000\n",
            "6/6 - 0s - loss: 542639.4375 - val_loss: 633281.0625\n",
            "Epoch 414/1000\n",
            "6/6 - 0s - loss: 511244.5625 - val_loss: 577196.1875\n",
            "Epoch 415/1000\n",
            "6/6 - 0s - loss: 483443.8125 - val_loss: 470413.5312\n",
            "Epoch 416/1000\n",
            "6/6 - 0s - loss: 444323.5312 - val_loss: 404116.1562\n",
            "Epoch 417/1000\n",
            "6/6 - 0s - loss: 377769.1562 - val_loss: 366065.5625\n",
            "Epoch 418/1000\n",
            "6/6 - 0s - loss: 334164.9375 - val_loss: 273118.9062\n",
            "Epoch 419/1000\n",
            "6/6 - 0s - loss: 284254.4062 - val_loss: 225101.2812\n",
            "Epoch 420/1000\n",
            "6/6 - 0s - loss: 231968.6562 - val_loss: 173991.1875\n",
            "Epoch 421/1000\n",
            "6/6 - 0s - loss: 198051.0312 - val_loss: 213492.4375\n",
            "Epoch 422/1000\n",
            "6/6 - 0s - loss: 170064.3281 - val_loss: 249174.7500\n",
            "Epoch 423/1000\n",
            "6/6 - 0s - loss: 143191.7031 - val_loss: 180621.8750\n",
            "Epoch 424/1000\n",
            "6/6 - 0s - loss: 119176.1797 - val_loss: 159406.8438\n",
            "Epoch 425/1000\n",
            "6/6 - 0s - loss: 104002.5703 - val_loss: 157599.5781\n",
            "Epoch 426/1000\n",
            "6/6 - 0s - loss: 87525.4141 - val_loss: 146626.0000\n",
            "Epoch 427/1000\n",
            "6/6 - 0s - loss: 100433.9453 - val_loss: 162055.6562\n",
            "Epoch 428/1000\n",
            "6/6 - 0s - loss: 93416.4297 - val_loss: 148037.9062\n",
            "Epoch 429/1000\n",
            "6/6 - 0s - loss: 88524.6094 - val_loss: 147455.3438\n",
            "Epoch 430/1000\n",
            "6/6 - 0s - loss: 71413.2031 - val_loss: 157925.9688\n",
            "Epoch 431/1000\n",
            "6/6 - 0s - loss: 79282.1094 - val_loss: 134383.2500\n",
            "Epoch 432/1000\n",
            "6/6 - 0s - loss: 64998.8242 - val_loss: 131739.6094\n",
            "Epoch 433/1000\n",
            "6/6 - 0s - loss: 55962.4453 - val_loss: 115861.3281\n",
            "Epoch 434/1000\n",
            "6/6 - 0s - loss: 53157.6289 - val_loss: 118189.0938\n",
            "Epoch 435/1000\n",
            "6/6 - 0s - loss: 39492.5586 - val_loss: 99523.7812\n",
            "Epoch 436/1000\n",
            "6/6 - 0s - loss: 35711.4531 - val_loss: 127990.1875\n",
            "Epoch 437/1000\n",
            "6/6 - 0s - loss: 43554.6914 - val_loss: 120617.0781\n",
            "Epoch 438/1000\n",
            "6/6 - 0s - loss: 69776.3125 - val_loss: 97550.5078\n",
            "Epoch 439/1000\n",
            "6/6 - 0s - loss: 112269.9609 - val_loss: 260267.8750\n",
            "Epoch 440/1000\n",
            "6/6 - 0s - loss: 250348.3438 - val_loss: 234922.0000\n",
            "Epoch 441/1000\n",
            "6/6 - 0s - loss: 242867.5625 - val_loss: 193778.5000\n",
            "Epoch 442/1000\n",
            "6/6 - 0s - loss: 143107.4219 - val_loss: 115382.5312\n",
            "Epoch 443/1000\n",
            "6/6 - 0s - loss: 70572.3281 - val_loss: 116304.8125\n",
            "Epoch 444/1000\n",
            "6/6 - 0s - loss: 62578.6602 - val_loss: 100669.1719\n",
            "Epoch 445/1000\n",
            "6/6 - 0s - loss: 45414.1641 - val_loss: 87262.4141\n",
            "Epoch 446/1000\n",
            "6/6 - 0s - loss: 55252.6602 - val_loss: 79768.1484\n",
            "Epoch 447/1000\n",
            "6/6 - 0s - loss: 32326.4023 - val_loss: 107363.4609\n",
            "Epoch 448/1000\n",
            "6/6 - 0s - loss: 53802.7656 - val_loss: 93503.7812\n",
            "Epoch 449/1000\n",
            "6/6 - 0s - loss: 31576.8086 - val_loss: 81169.3594\n",
            "Epoch 450/1000\n",
            "6/6 - 0s - loss: 28074.1465 - val_loss: 103677.9688\n",
            "Epoch 451/1000\n",
            "6/6 - 0s - loss: 26170.7930 - val_loss: 91267.0469\n",
            "Epoch 452/1000\n",
            "6/6 - 0s - loss: 21792.2285 - val_loss: 72082.0312\n",
            "Epoch 453/1000\n",
            "6/6 - 0s - loss: 22232.8555 - val_loss: 91914.8281\n",
            "Epoch 454/1000\n",
            "6/6 - 0s - loss: 20134.2910 - val_loss: 72562.5703\n",
            "Epoch 455/1000\n",
            "6/6 - 0s - loss: 19127.9492 - val_loss: 98942.6328\n",
            "Epoch 456/1000\n",
            "6/6 - 0s - loss: 23991.8027 - val_loss: 105996.6641\n",
            "Epoch 457/1000\n",
            "6/6 - 0s - loss: 23246.4648 - val_loss: 64136.6797\n",
            "Epoch 458/1000\n",
            "6/6 - 0s - loss: 25949.5820 - val_loss: 62900.5195\n",
            "Epoch 459/1000\n",
            "6/6 - 0s - loss: 45000.1680 - val_loss: 83132.0703\n",
            "Epoch 460/1000\n",
            "6/6 - 0s - loss: 29482.1152 - val_loss: 97556.5156\n",
            "Epoch 461/1000\n",
            "6/6 - 0s - loss: 27470.8379 - val_loss: 98978.3594\n",
            "Epoch 462/1000\n",
            "6/6 - 0s - loss: 22460.8711 - val_loss: 86881.6797\n",
            "Epoch 463/1000\n",
            "6/6 - 0s - loss: 20820.4609 - val_loss: 95071.8203\n",
            "Epoch 464/1000\n",
            "6/6 - 0s - loss: 19943.0137 - val_loss: 91250.1875\n",
            "Epoch 465/1000\n",
            "6/6 - 0s - loss: 19166.1074 - val_loss: 82483.8594\n",
            "Epoch 466/1000\n",
            "6/6 - 0s - loss: 18961.4805 - val_loss: 136950.2188\n",
            "Epoch 467/1000\n",
            "6/6 - 0s - loss: 17945.6348 - val_loss: 70254.8594\n",
            "Epoch 468/1000\n",
            "6/6 - 0s - loss: 18694.8789 - val_loss: 102312.3906\n",
            "Epoch 469/1000\n",
            "6/6 - 0s - loss: 18025.5156 - val_loss: 65808.4062\n",
            "Epoch 470/1000\n",
            "6/6 - 0s - loss: 18434.6738 - val_loss: 63518.0156\n",
            "Epoch 471/1000\n",
            "6/6 - 0s - loss: 21369.3867 - val_loss: 61824.1875\n",
            "Epoch 472/1000\n",
            "6/6 - 0s - loss: 32244.4980 - val_loss: 58068.5703\n",
            "Epoch 473/1000\n",
            "6/6 - 0s - loss: 25293.3828 - val_loss: 56712.4688\n",
            "Epoch 474/1000\n",
            "6/6 - 0s - loss: 26258.7578 - val_loss: 57928.9609\n",
            "Epoch 475/1000\n",
            "6/6 - 0s - loss: 22372.2012 - val_loss: 62735.9688\n",
            "Epoch 476/1000\n",
            "6/6 - 0s - loss: 17270.3906 - val_loss: 77466.1250\n",
            "Epoch 477/1000\n",
            "6/6 - 0s - loss: 18116.3828 - val_loss: 61486.6797\n",
            "Epoch 478/1000\n",
            "6/6 - 0s - loss: 18268.8301 - val_loss: 68189.8594\n",
            "Epoch 479/1000\n",
            "6/6 - 0s - loss: 41947.4062 - val_loss: 69918.5078\n",
            "Epoch 480/1000\n",
            "6/6 - 0s - loss: 28579.1016 - val_loss: 91327.6250\n",
            "Epoch 481/1000\n",
            "6/6 - 0s - loss: 32782.9258 - val_loss: 68025.0859\n",
            "Epoch 482/1000\n",
            "6/6 - 0s - loss: 24150.3867 - val_loss: 77152.8281\n",
            "Epoch 483/1000\n",
            "6/6 - 0s - loss: 19184.0898 - val_loss: 76445.5547\n",
            "Epoch 484/1000\n",
            "6/6 - 0s - loss: 17378.1367 - val_loss: 71255.9688\n",
            "Epoch 485/1000\n",
            "6/6 - 0s - loss: 16507.8828 - val_loss: 70024.7812\n",
            "Epoch 486/1000\n",
            "6/6 - 0s - loss: 15505.2783 - val_loss: 95330.1406\n",
            "Epoch 487/1000\n",
            "6/6 - 0s - loss: 15073.0684 - val_loss: 61672.2266\n",
            "Epoch 488/1000\n",
            "6/6 - 0s - loss: 14781.9609 - val_loss: 60456.3047\n",
            "Epoch 489/1000\n",
            "6/6 - 0s - loss: 14086.3594 - val_loss: 59744.3867\n",
            "Epoch 490/1000\n",
            "6/6 - 0s - loss: 14074.4854 - val_loss: 65913.6328\n",
            "Epoch 491/1000\n",
            "6/6 - 0s - loss: 13159.8447 - val_loss: 62542.0000\n",
            "Epoch 492/1000\n",
            "6/6 - 0s - loss: 13037.7686 - val_loss: 65764.5703\n",
            "Epoch 493/1000\n",
            "6/6 - 0s - loss: 12627.2285 - val_loss: 78077.3672\n",
            "Epoch 494/1000\n",
            "6/6 - 0s - loss: 12238.1260 - val_loss: 61830.3594\n",
            "Epoch 495/1000\n",
            "6/6 - 0s - loss: 11648.6074 - val_loss: 61458.2656\n",
            "Epoch 496/1000\n",
            "6/6 - 0s - loss: 12467.1895 - val_loss: 55191.4922\n",
            "Epoch 497/1000\n",
            "6/6 - 0s - loss: 14892.3047 - val_loss: 53999.9883\n",
            "Epoch 498/1000\n",
            "6/6 - 0s - loss: 20283.6621 - val_loss: 57551.5000\n",
            "Epoch 499/1000\n",
            "6/6 - 0s - loss: 40628.3633 - val_loss: 79962.8281\n",
            "Epoch 500/1000\n",
            "6/6 - 0s - loss: 32539.3516 - val_loss: 99829.0781\n",
            "Epoch 501/1000\n",
            "6/6 - 0s - loss: 32251.4375 - val_loss: 80862.7969\n",
            "Epoch 502/1000\n",
            "6/6 - 0s - loss: 56726.7188 - val_loss: 63305.2109\n",
            "Epoch 503/1000\n",
            "6/6 - 0s - loss: 186173.5156 - val_loss: 417414.9375\n",
            "Epoch 504/1000\n",
            "6/6 - 0s - loss: 443505.4375 - val_loss: 533480.6250\n",
            "Epoch 505/1000\n",
            "6/6 - 0s - loss: 514353.9688 - val_loss: 455370.4062\n",
            "Epoch 506/1000\n",
            "6/6 - 0s - loss: 453440.5000 - val_loss: 503520.0000\n",
            "Epoch 507/1000\n",
            "6/6 - 0s - loss: 335163.6250 - val_loss: 268643.8438\n",
            "Epoch 508/1000\n",
            "6/6 - 0s - loss: 296305.8438 - val_loss: 208346.7812\n",
            "Epoch 509/1000\n",
            "6/6 - 0s - loss: 182842.6875 - val_loss: 143139.2656\n",
            "Epoch 510/1000\n",
            "6/6 - 0s - loss: 632512.5625 - val_loss: 536664.1875\n",
            "Epoch 511/1000\n",
            "6/6 - 0s - loss: 589872.4375 - val_loss: 742643.1250\n",
            "Epoch 512/1000\n",
            "6/6 - 0s - loss: 755934.7500 - val_loss: 803153.0000\n",
            "Epoch 513/1000\n",
            "6/6 - 0s - loss: 593557.1250 - val_loss: 914767.6250\n",
            "Epoch 514/1000\n",
            "6/6 - 0s - loss: 688069.4375 - val_loss: 856378.4375\n",
            "Epoch 515/1000\n",
            "6/6 - 0s - loss: 631281.1875 - val_loss: 798163.8125\n",
            "Epoch 516/1000\n",
            "6/6 - 0s - loss: 610757.1250 - val_loss: 1479865.2500\n",
            "Epoch 517/1000\n",
            "6/6 - 0s - loss: 660453.1875 - val_loss: 806442.0625\n",
            "Epoch 518/1000\n",
            "6/6 - 0s - loss: 576644.5625 - val_loss: 695664.6875\n",
            "Epoch 519/1000\n",
            "6/6 - 0s - loss: 513236.7188 - val_loss: 636042.1250\n",
            "Epoch 520/1000\n",
            "6/6 - 0s - loss: 488698.4688 - val_loss: 602797.4375\n",
            "Epoch 521/1000\n",
            "6/6 - 0s - loss: 463278.3750 - val_loss: 511960.1250\n",
            "Epoch 522/1000\n",
            "6/6 - 0s - loss: 503668.1250 - val_loss: 502834.4375\n",
            "Epoch 523/1000\n",
            "6/6 - 0s - loss: 470986.4688 - val_loss: 474034.0625\n",
            "Epoch 524/1000\n",
            "6/6 - 0s - loss: 461146.0312 - val_loss: 405580.8750\n",
            "Epoch 525/1000\n",
            "6/6 - 0s - loss: 468427.4375 - val_loss: 256912.6719\n",
            "Epoch 526/1000\n",
            "6/6 - 0s - loss: 270976.4062 - val_loss: 329036.2500\n",
            "Epoch 527/1000\n",
            "6/6 - 0s - loss: 346927.3750 - val_loss: 382444.3750\n",
            "Epoch 528/1000\n",
            "6/6 - 0s - loss: 245419.6719 - val_loss: 257181.8438\n",
            "Epoch 529/1000\n",
            "6/6 - 0s - loss: 205067.4062 - val_loss: 314624.7500\n",
            "Epoch 530/1000\n",
            "6/6 - 0s - loss: 163469.8594 - val_loss: 299500.3438\n",
            "Epoch 531/1000\n",
            "6/6 - 0s - loss: 175234.6406 - val_loss: 332815.3750\n",
            "Epoch 532/1000\n",
            "6/6 - 0s - loss: 154987.7969 - val_loss: 280451.6875\n",
            "Epoch 533/1000\n",
            "6/6 - 0s - loss: 149822.4688 - val_loss: 322858.2812\n",
            "Epoch 534/1000\n",
            "6/6 - 0s - loss: 138182.4219 - val_loss: 295330.5625\n",
            "Epoch 535/1000\n",
            "6/6 - 0s - loss: 127463.9297 - val_loss: 294596.9688\n",
            "Epoch 536/1000\n",
            "6/6 - 0s - loss: 119302.3125 - val_loss: 265575.6875\n",
            "Epoch 537/1000\n",
            "6/6 - 0s - loss: 114218.2266 - val_loss: 261099.0000\n",
            "Epoch 538/1000\n",
            "6/6 - 0s - loss: 106832.6172 - val_loss: 290128.9062\n",
            "Epoch 539/1000\n",
            "6/6 - 0s - loss: 117533.0938 - val_loss: 207728.4531\n",
            "Epoch 540/1000\n",
            "6/6 - 0s - loss: 124233.5938 - val_loss: 286940.1875\n",
            "Epoch 541/1000\n",
            "6/6 - 0s - loss: 111716.5703 - val_loss: 304359.0625\n",
            "Epoch 542/1000\n",
            "6/6 - 0s - loss: 104557.1094 - val_loss: 224552.9375\n",
            "Epoch 543/1000\n",
            "6/6 - 0s - loss: 101022.9219 - val_loss: 302804.3438\n",
            "Epoch 544/1000\n",
            "6/6 - 0s - loss: 97809.7344 - val_loss: 226713.9062\n",
            "Epoch 545/1000\n",
            "6/6 - 0s - loss: 96390.7266 - val_loss: 253311.7812\n",
            "Epoch 546/1000\n",
            "6/6 - 0s - loss: 89177.7500 - val_loss: 319401.6875\n",
            "Epoch 547/1000\n",
            "6/6 - 0s - loss: 91603.1719 - val_loss: 188533.2188\n",
            "Epoch 548/1000\n",
            "6/6 - 0s - loss: 80253.0000 - val_loss: 490874.9688\n",
            "Epoch 549/1000\n",
            "6/6 - 0s - loss: 84121.2188 - val_loss: 809067.3750\n",
            "Epoch 550/1000\n",
            "6/6 - 0s - loss: 191150.1094 - val_loss: 246713.8125\n",
            "Epoch 551/1000\n",
            "6/6 - 0s - loss: 250740.3281 - val_loss: 232074.8750\n",
            "Epoch 552/1000\n",
            "6/6 - 0s - loss: 232974.6094 - val_loss: 212457.9062\n",
            "Epoch 553/1000\n",
            "6/6 - 0s - loss: 178988.2188 - val_loss: 156552.9844\n",
            "Epoch 554/1000\n",
            "6/6 - 0s - loss: 314849.6250 - val_loss: 414354.8750\n",
            "Epoch 555/1000\n",
            "6/6 - 0s - loss: 539488.0000 - val_loss: 591096.4375\n",
            "Epoch 556/1000\n",
            "6/6 - 0s - loss: 538126.4375 - val_loss: 699768.2500\n",
            "Epoch 557/1000\n",
            "6/6 - 0s - loss: 548141.3750 - val_loss: 790523.8125\n",
            "Epoch 558/1000\n",
            "6/6 - 0s - loss: 569847.7500 - val_loss: 733962.2500\n",
            "Epoch 559/1000\n",
            "6/6 - 0s - loss: 507010.9062 - val_loss: 649575.9375\n",
            "Epoch 560/1000\n",
            "6/6 - 0s - loss: 514404.5000 - val_loss: 642916.3750\n",
            "Epoch 561/1000\n",
            "6/6 - 0s - loss: 499525.4688 - val_loss: 584921.5000\n",
            "Epoch 562/1000\n",
            "6/6 - 0s - loss: 474278.9375 - val_loss: 500817.8125\n",
            "Epoch 563/1000\n",
            "6/6 - 0s - loss: 451156.7188 - val_loss: 481029.7500\n",
            "Epoch 564/1000\n",
            "6/6 - 0s - loss: 440138.0312 - val_loss: 440441.1562\n",
            "Epoch 565/1000\n",
            "6/6 - 0s - loss: 432341.5938 - val_loss: 410449.3750\n",
            "Epoch 566/1000\n",
            "6/6 - 0s - loss: 419161.4375 - val_loss: 409804.1875\n",
            "Epoch 567/1000\n",
            "6/6 - 0s - loss: 406940.7188 - val_loss: 328562.2188\n",
            "Epoch 568/1000\n",
            "6/6 - 0s - loss: 364458.7500 - val_loss: 342376.0625\n",
            "Epoch 569/1000\n",
            "6/6 - 0s - loss: 358225.0000 - val_loss: 386642.9375\n",
            "Epoch 570/1000\n",
            "6/6 - 0s - loss: 378274.7188 - val_loss: 296313.8125\n",
            "Epoch 571/1000\n",
            "6/6 - 0s - loss: 345623.9375 - val_loss: 247945.4531\n",
            "Epoch 572/1000\n",
            "6/6 - 0s - loss: 319130.4375 - val_loss: 224124.7969\n",
            "Epoch 573/1000\n",
            "6/6 - 0s - loss: 289046.2812 - val_loss: 221670.2812\n",
            "Epoch 574/1000\n",
            "6/6 - 0s - loss: 277050.0938 - val_loss: 243365.6094\n",
            "Epoch 575/1000\n",
            "6/6 - 0s - loss: 259320.3594 - val_loss: 209896.6719\n",
            "Epoch 576/1000\n",
            "6/6 - 0s - loss: 238452.9375 - val_loss: 191642.9375\n",
            "Epoch 577/1000\n",
            "6/6 - 0s - loss: 217465.8125 - val_loss: 183592.8438\n",
            "Epoch 578/1000\n",
            "6/6 - 0s - loss: 165107.3125 - val_loss: 274888.3750\n",
            "Epoch 579/1000\n",
            "6/6 - 0s - loss: 218714.7031 - val_loss: 191178.7812\n",
            "Epoch 580/1000\n",
            "6/6 - 0s - loss: 211641.5312 - val_loss: 198879.1562\n",
            "Epoch 581/1000\n",
            "6/6 - 0s - loss: 213401.1094 - val_loss: 187325.0156\n",
            "Epoch 582/1000\n",
            "6/6 - 0s - loss: 178001.0312 - val_loss: 201728.5781\n",
            "Epoch 583/1000\n",
            "6/6 - 0s - loss: 149794.2500 - val_loss: 239087.3594\n",
            "Epoch 584/1000\n",
            "6/6 - 0s - loss: 109622.4141 - val_loss: 174078.9844\n",
            "Epoch 585/1000\n",
            "6/6 - 0s - loss: 106015.5312 - val_loss: 168562.5312\n",
            "Epoch 586/1000\n",
            "6/6 - 0s - loss: 93033.6172 - val_loss: 165490.7500\n",
            "Epoch 587/1000\n",
            "6/6 - 0s - loss: 87242.1719 - val_loss: 156129.8125\n",
            "Epoch 588/1000\n",
            "6/6 - 0s - loss: 83958.5547 - val_loss: 160234.0312\n",
            "Epoch 589/1000\n",
            "6/6 - 0s - loss: 106973.1953 - val_loss: 155361.9688\n",
            "Epoch 590/1000\n",
            "6/6 - 0s - loss: 101596.5078 - val_loss: 145860.6094\n",
            "Epoch 591/1000\n",
            "6/6 - 0s - loss: 75098.1719 - val_loss: 134825.5625\n",
            "Epoch 592/1000\n",
            "6/6 - 0s - loss: 78753.3047 - val_loss: 138081.1875\n",
            "Epoch 593/1000\n",
            "6/6 - 0s - loss: 79494.4375 - val_loss: 138219.8750\n",
            "Epoch 594/1000\n",
            "6/6 - 0s - loss: 80863.6406 - val_loss: 136749.4219\n",
            "Epoch 595/1000\n",
            "6/6 - 0s - loss: 79040.3594 - val_loss: 136526.0469\n",
            "Epoch 596/1000\n",
            "6/6 - 0s - loss: 79912.1172 - val_loss: 131045.1250\n",
            "Epoch 597/1000\n",
            "6/6 - 0s - loss: 70152.1094 - val_loss: 131268.2031\n",
            "Epoch 598/1000\n",
            "6/6 - 0s - loss: 69955.7891 - val_loss: 125036.1484\n",
            "Epoch 599/1000\n",
            "6/6 - 0s - loss: 67023.6953 - val_loss: 138965.7188\n",
            "Epoch 600/1000\n",
            "6/6 - 0s - loss: 75869.3828 - val_loss: 136284.0000\n",
            "Epoch 601/1000\n",
            "6/6 - 0s - loss: 96269.3125 - val_loss: 168321.8594\n",
            "Epoch 602/1000\n",
            "6/6 - 0s - loss: 95725.4375 - val_loss: 128523.5938\n",
            "Epoch 603/1000\n",
            "6/6 - 0s - loss: 80586.7812 - val_loss: 125256.1875\n",
            "Epoch 604/1000\n",
            "6/6 - 0s - loss: 88969.0625 - val_loss: 124141.5625\n",
            "Epoch 605/1000\n",
            "6/6 - 0s - loss: 72365.0078 - val_loss: 127805.0391\n",
            "Epoch 606/1000\n",
            "6/6 - 0s - loss: 65217.5430 - val_loss: 122663.8516\n",
            "Epoch 607/1000\n",
            "6/6 - 0s - loss: 61678.1406 - val_loss: 123249.1328\n",
            "Epoch 608/1000\n",
            "6/6 - 0s - loss: 64811.4492 - val_loss: 122494.3281\n",
            "Epoch 609/1000\n",
            "6/6 - 0s - loss: 57408.0938 - val_loss: 116160.0469\n",
            "Epoch 610/1000\n",
            "6/6 - 0s - loss: 54180.5469 - val_loss: 124462.0938\n",
            "Epoch 611/1000\n",
            "6/6 - 0s - loss: 54010.2734 - val_loss: 113406.5000\n",
            "Epoch 612/1000\n",
            "6/6 - 0s - loss: 51441.9727 - val_loss: 133003.1875\n",
            "Epoch 613/1000\n",
            "6/6 - 0s - loss: 49071.0039 - val_loss: 111845.4062\n",
            "Epoch 614/1000\n",
            "6/6 - 0s - loss: 45897.4805 - val_loss: 105863.3438\n",
            "Epoch 615/1000\n",
            "6/6 - 0s - loss: 58140.3672 - val_loss: 125712.7578\n",
            "Epoch 616/1000\n",
            "6/6 - 0s - loss: 75158.8594 - val_loss: 186992.8438\n",
            "Epoch 617/1000\n",
            "6/6 - 0s - loss: 54394.9766 - val_loss: 109548.2344\n",
            "Epoch 618/1000\n",
            "6/6 - 0s - loss: 56517.9883 - val_loss: 120008.1328\n",
            "Epoch 619/1000\n",
            "6/6 - 0s - loss: 90073.6250 - val_loss: 193224.4375\n",
            "Epoch 620/1000\n",
            "6/6 - 0s - loss: 209295.7969 - val_loss: 182794.1094\n",
            "Epoch 621/1000\n",
            "6/6 - 0s - loss: 154654.9062 - val_loss: 123996.3516\n",
            "Epoch 622/1000\n",
            "6/6 - 0s - loss: 105058.3281 - val_loss: 216229.2500\n",
            "Epoch 623/1000\n",
            "6/6 - 0s - loss: 122109.6719 - val_loss: 100926.1875\n",
            "Epoch 624/1000\n",
            "6/6 - 0s - loss: 77587.8281 - val_loss: 96887.2344\n",
            "Epoch 625/1000\n",
            "6/6 - 0s - loss: 41909.7422 - val_loss: 85926.4062\n",
            "Epoch 626/1000\n",
            "6/6 - 0s - loss: 49468.5195 - val_loss: 129995.9531\n",
            "Epoch 627/1000\n",
            "6/6 - 0s - loss: 106927.5078 - val_loss: 175517.6719\n",
            "Epoch 628/1000\n",
            "6/6 - 0s - loss: 225996.0156 - val_loss: 192492.9375\n",
            "Epoch 629/1000\n",
            "6/6 - 0s - loss: 206054.9531 - val_loss: 159766.8438\n",
            "Epoch 630/1000\n",
            "6/6 - 0s - loss: 123254.7188 - val_loss: 132767.7812\n",
            "Epoch 631/1000\n",
            "6/6 - 0s - loss: 127964.5391 - val_loss: 152931.1250\n",
            "Epoch 632/1000\n",
            "6/6 - 0s - loss: 78902.8984 - val_loss: 113091.0547\n",
            "Epoch 633/1000\n",
            "6/6 - 0s - loss: 85042.5312 - val_loss: 99579.9219\n",
            "Epoch 634/1000\n",
            "6/6 - 0s - loss: 60578.7617 - val_loss: 88301.4375\n",
            "Epoch 635/1000\n",
            "6/6 - 0s - loss: 61464.6250 - val_loss: 90782.4766\n",
            "Epoch 636/1000\n",
            "6/6 - 0s - loss: 58076.2734 - val_loss: 103613.5312\n",
            "Epoch 637/1000\n",
            "6/6 - 0s - loss: 71613.1797 - val_loss: 113630.2969\n",
            "Epoch 638/1000\n",
            "6/6 - 0s - loss: 45164.0508 - val_loss: 83333.4609\n",
            "Epoch 639/1000\n",
            "6/6 - 0s - loss: 38233.1055 - val_loss: 73466.1406\n",
            "Epoch 640/1000\n",
            "6/6 - 0s - loss: 32267.1504 - val_loss: 72674.4922\n",
            "Epoch 641/1000\n",
            "6/6 - 0s - loss: 29605.8496 - val_loss: 67523.0312\n",
            "Epoch 642/1000\n",
            "6/6 - 0s - loss: 30653.8633 - val_loss: 72226.0000\n",
            "Epoch 643/1000\n",
            "6/6 - 0s - loss: 30816.4648 - val_loss: 108023.7500\n",
            "Epoch 644/1000\n",
            "6/6 - 0s - loss: 31066.7949 - val_loss: 64142.7305\n",
            "Epoch 645/1000\n",
            "6/6 - 0s - loss: 40155.3281 - val_loss: 92061.6719\n",
            "Epoch 646/1000\n",
            "6/6 - 0s - loss: 118391.8984 - val_loss: 108926.9609\n",
            "Epoch 647/1000\n",
            "6/6 - 0s - loss: 95380.8828 - val_loss: 91164.6719\n",
            "Epoch 648/1000\n",
            "6/6 - 0s - loss: 60993.0703 - val_loss: 70480.3984\n",
            "Epoch 649/1000\n",
            "6/6 - 0s - loss: 46098.0039 - val_loss: 70810.7422\n",
            "Epoch 650/1000\n",
            "6/6 - 0s - loss: 34333.6758 - val_loss: 77118.4844\n",
            "Epoch 651/1000\n",
            "6/6 - 0s - loss: 26057.1602 - val_loss: 96275.2188\n",
            "Epoch 652/1000\n",
            "6/6 - 0s - loss: 27280.3105 - val_loss: 64750.7148\n",
            "Epoch 653/1000\n",
            "6/6 - 0s - loss: 35243.4531 - val_loss: 129059.8594\n",
            "Epoch 654/1000\n",
            "6/6 - 0s - loss: 34276.7852 - val_loss: 73647.6875\n",
            "Epoch 655/1000\n",
            "6/6 - 0s - loss: 39256.3086 - val_loss: 178490.3125\n",
            "Epoch 656/1000\n",
            "6/6 - 0s - loss: 41267.0586 - val_loss: 60062.5117\n",
            "Epoch 657/1000\n",
            "6/6 - 0s - loss: 47538.4766 - val_loss: 68899.7656\n",
            "Epoch 658/1000\n",
            "6/6 - 0s - loss: 162498.1875 - val_loss: 311000.3750\n",
            "Epoch 659/1000\n",
            "6/6 - 0s - loss: 362778.2812 - val_loss: 443041.4375\n",
            "Epoch 660/1000\n",
            "6/6 - 0s - loss: 428203.4375 - val_loss: 552078.7500\n",
            "Epoch 661/1000\n",
            "6/6 - 0s - loss: 382808.0625 - val_loss: 427292.2500\n",
            "Epoch 662/1000\n",
            "6/6 - 0s - loss: 333518.3125 - val_loss: 391712.1562\n",
            "Epoch 663/1000\n",
            "6/6 - 0s - loss: 283556.5000 - val_loss: 326465.3750\n",
            "Epoch 664/1000\n",
            "6/6 - 0s - loss: 215693.7344 - val_loss: 348813.6875\n",
            "Epoch 665/1000\n",
            "6/6 - 0s - loss: 180558.7031 - val_loss: 264397.0000\n",
            "Epoch 666/1000\n",
            "6/6 - 0s - loss: 179892.4375 - val_loss: 240894.6250\n",
            "Epoch 667/1000\n",
            "6/6 - 0s - loss: 137669.0469 - val_loss: 265816.2812\n",
            "Epoch 668/1000\n",
            "6/6 - 0s - loss: 149588.4062 - val_loss: 220680.4375\n",
            "Epoch 669/1000\n",
            "6/6 - 0s - loss: 127302.0859 - val_loss: 182945.2656\n",
            "Epoch 670/1000\n",
            "6/6 - 0s - loss: 118210.3594 - val_loss: 196314.3125\n",
            "Epoch 671/1000\n",
            "6/6 - 0s - loss: 86101.8359 - val_loss: 214967.5469\n",
            "Epoch 672/1000\n",
            "6/6 - 0s - loss: 77998.6875 - val_loss: 234159.9688\n",
            "Epoch 673/1000\n",
            "6/6 - 0s - loss: 67508.9375 - val_loss: 170248.7188\n",
            "Epoch 674/1000\n",
            "6/6 - 0s - loss: 65241.8789 - val_loss: 154050.7500\n",
            "Epoch 675/1000\n",
            "6/6 - 0s - loss: 57740.6719 - val_loss: 238770.9375\n",
            "Epoch 676/1000\n",
            "6/6 - 0s - loss: 50073.5469 - val_loss: 186710.5781\n",
            "Epoch 677/1000\n",
            "6/6 - 0s - loss: 44349.2695 - val_loss: 188765.3125\n",
            "Epoch 678/1000\n",
            "6/6 - 0s - loss: 40707.5312 - val_loss: 102220.2188\n",
            "Epoch 679/1000\n",
            "6/6 - 0s - loss: 39307.8281 - val_loss: 81549.3438\n",
            "Epoch 680/1000\n",
            "6/6 - 0s - loss: 32067.3730 - val_loss: 95205.6797\n",
            "Epoch 681/1000\n",
            "6/6 - 0s - loss: 27379.7891 - val_loss: 121762.7500\n",
            "Epoch 682/1000\n",
            "6/6 - 0s - loss: 24138.7480 - val_loss: 168926.2500\n",
            "Epoch 683/1000\n",
            "6/6 - 0s - loss: 22963.4336 - val_loss: 102698.5312\n",
            "Epoch 684/1000\n",
            "6/6 - 0s - loss: 23290.8867 - val_loss: 186980.3750\n",
            "Epoch 685/1000\n",
            "6/6 - 0s - loss: 24893.9551 - val_loss: 91281.3281\n",
            "Epoch 686/1000\n",
            "6/6 - 0s - loss: 20837.6387 - val_loss: 152322.2812\n",
            "Epoch 687/1000\n",
            "6/6 - 0s - loss: 21115.5137 - val_loss: 127549.8281\n",
            "Epoch 688/1000\n",
            "6/6 - 0s - loss: 21724.8496 - val_loss: 118342.9375\n",
            "Epoch 689/1000\n",
            "6/6 - 0s - loss: 19292.3359 - val_loss: 133025.6875\n",
            "Epoch 690/1000\n",
            "6/6 - 0s - loss: 17148.7949 - val_loss: 126139.5859\n",
            "Epoch 691/1000\n",
            "6/6 - 0s - loss: 18897.9414 - val_loss: 139780.3438\n",
            "Epoch 692/1000\n",
            "6/6 - 0s - loss: 18222.0508 - val_loss: 134391.4688\n",
            "Epoch 693/1000\n",
            "6/6 - 0s - loss: 17087.2578 - val_loss: 106295.0391\n",
            "Epoch 694/1000\n",
            "6/6 - 0s - loss: 16622.0938 - val_loss: 138250.8125\n",
            "Epoch 695/1000\n",
            "6/6 - 0s - loss: 15812.5010 - val_loss: 109497.8516\n",
            "Epoch 696/1000\n",
            "6/6 - 0s - loss: 15587.1445 - val_loss: 118802.0234\n",
            "Epoch 697/1000\n",
            "6/6 - 0s - loss: 15265.9590 - val_loss: 145867.1719\n",
            "Epoch 698/1000\n",
            "6/6 - 0s - loss: 14847.8662 - val_loss: 105612.9297\n",
            "Epoch 699/1000\n",
            "6/6 - 0s - loss: 15965.6992 - val_loss: 112929.1797\n",
            "Epoch 700/1000\n",
            "6/6 - 0s - loss: 15238.2607 - val_loss: 155410.1250\n",
            "Epoch 701/1000\n",
            "6/6 - 0s - loss: 16127.6387 - val_loss: 93260.9688\n",
            "Epoch 702/1000\n",
            "6/6 - 0s - loss: 19311.5566 - val_loss: 113381.6875\n",
            "Epoch 703/1000\n",
            "6/6 - 0s - loss: 24449.5430 - val_loss: 113260.3125\n",
            "Epoch 704/1000\n",
            "6/6 - 0s - loss: 17554.8516 - val_loss: 101759.5469\n",
            "Epoch 705/1000\n",
            "6/6 - 0s - loss: 19076.2852 - val_loss: 178846.5469\n",
            "Epoch 706/1000\n",
            "6/6 - 0s - loss: 27457.2754 - val_loss: 67922.1562\n",
            "Epoch 707/1000\n",
            "6/6 - 0s - loss: 23652.8262 - val_loss: 178798.3594\n",
            "Epoch 708/1000\n",
            "6/6 - 0s - loss: 19022.9707 - val_loss: 76713.1719\n",
            "Epoch 709/1000\n",
            "6/6 - 0s - loss: 20525.3027 - val_loss: 234587.9531\n",
            "Epoch 710/1000\n",
            "6/6 - 0s - loss: 21927.1719 - val_loss: 67453.9688\n",
            "Epoch 711/1000\n",
            "6/6 - 0s - loss: 20465.1699 - val_loss: 124649.7812\n",
            "Epoch 712/1000\n",
            "6/6 - 0s - loss: 16665.1211 - val_loss: 109082.7656\n",
            "Epoch 713/1000\n",
            "6/6 - 0s - loss: 16818.9531 - val_loss: 123680.9297\n",
            "Epoch 714/1000\n",
            "6/6 - 0s - loss: 17567.1035 - val_loss: 91263.5781\n",
            "Epoch 715/1000\n",
            "6/6 - 0s - loss: 19520.4297 - val_loss: 170761.2656\n",
            "Epoch 716/1000\n",
            "6/6 - 0s - loss: 14221.7520 - val_loss: 120517.9453\n",
            "Epoch 717/1000\n",
            "6/6 - 0s - loss: 14110.6680 - val_loss: 137238.7031\n",
            "Epoch 718/1000\n",
            "6/6 - 0s - loss: 14268.2832 - val_loss: 110631.4375\n",
            "Epoch 719/1000\n",
            "6/6 - 0s - loss: 14268.9004 - val_loss: 122308.1797\n",
            "Epoch 720/1000\n",
            "6/6 - 0s - loss: 13726.2773 - val_loss: 164213.9688\n",
            "Epoch 721/1000\n",
            "6/6 - 0s - loss: 13976.1914 - val_loss: 122277.4531\n",
            "Epoch 722/1000\n",
            "6/6 - 0s - loss: 14226.9209 - val_loss: 91947.6406\n",
            "Epoch 723/1000\n",
            "6/6 - 0s - loss: 15731.6191 - val_loss: 149428.1719\n",
            "Epoch 724/1000\n",
            "6/6 - 0s - loss: 15691.7793 - val_loss: 99415.2578\n",
            "Epoch 725/1000\n",
            "6/6 - 0s - loss: 14740.1250 - val_loss: 126803.1328\n",
            "Epoch 726/1000\n",
            "6/6 - 0s - loss: 14705.1562 - val_loss: 108234.2266\n",
            "Epoch 727/1000\n",
            "6/6 - 0s - loss: 14421.9453 - val_loss: 124296.5234\n",
            "Epoch 728/1000\n",
            "6/6 - 0s - loss: 14476.0312 - val_loss: 159152.9062\n",
            "Epoch 729/1000\n",
            "6/6 - 0s - loss: 18273.3730 - val_loss: 97747.3125\n",
            "Epoch 730/1000\n",
            "6/6 - 0s - loss: 19465.2422 - val_loss: 83050.8750\n",
            "Epoch 731/1000\n",
            "6/6 - 0s - loss: 23721.1602 - val_loss: 132962.5469\n",
            "Epoch 732/1000\n",
            "6/6 - 0s - loss: 18928.6367 - val_loss: 147677.7656\n",
            "Epoch 733/1000\n",
            "6/6 - 0s - loss: 14927.6797 - val_loss: 96937.9844\n",
            "Epoch 734/1000\n",
            "6/6 - 0s - loss: 14389.8447 - val_loss: 132978.2812\n",
            "Epoch 735/1000\n",
            "6/6 - 0s - loss: 14751.0576 - val_loss: 132786.9844\n",
            "Epoch 736/1000\n",
            "6/6 - 0s - loss: 14434.0986 - val_loss: 104270.7969\n",
            "Epoch 737/1000\n",
            "6/6 - 0s - loss: 13206.1475 - val_loss: 143376.5625\n",
            "Epoch 738/1000\n",
            "6/6 - 0s - loss: 14911.6377 - val_loss: 119291.4375\n",
            "Epoch 739/1000\n",
            "6/6 - 0s - loss: 15818.1943 - val_loss: 106415.4844\n",
            "Epoch 740/1000\n",
            "6/6 - 0s - loss: 15524.8555 - val_loss: 141429.7656\n",
            "Epoch 741/1000\n",
            "6/6 - 0s - loss: 13428.1445 - val_loss: 125215.8281\n",
            "Epoch 742/1000\n",
            "6/6 - 0s - loss: 13932.2031 - val_loss: 104921.1875\n",
            "Epoch 743/1000\n",
            "6/6 - 0s - loss: 14884.6572 - val_loss: 97179.1406\n",
            "Epoch 744/1000\n",
            "6/6 - 0s - loss: 14792.4043 - val_loss: 157112.2812\n",
            "Epoch 745/1000\n",
            "6/6 - 0s - loss: 24302.4980 - val_loss: 60909.2969\n",
            "Epoch 746/1000\n",
            "6/6 - 0s - loss: 19735.6641 - val_loss: 73914.2188\n",
            "Epoch 747/1000\n",
            "6/6 - 0s - loss: 24190.8496 - val_loss: 116427.2031\n",
            "Epoch 748/1000\n",
            "6/6 - 0s - loss: 23014.8379 - val_loss: 56175.4609\n",
            "Epoch 749/1000\n",
            "6/6 - 0s - loss: 24146.8145 - val_loss: 94087.1484\n",
            "Epoch 750/1000\n",
            "6/6 - 0s - loss: 28522.3691 - val_loss: 70154.9453\n",
            "Epoch 751/1000\n",
            "6/6 - 0s - loss: 32783.3281 - val_loss: 242809.9688\n",
            "Epoch 752/1000\n",
            "6/6 - 0s - loss: 29790.4941 - val_loss: 67241.3281\n",
            "Epoch 753/1000\n",
            "6/6 - 0s - loss: 50134.7070 - val_loss: 182934.6250\n",
            "Epoch 754/1000\n",
            "6/6 - 0s - loss: 45697.6914 - val_loss: 73126.7969\n",
            "Epoch 755/1000\n",
            "6/6 - 0s - loss: 57508.9648 - val_loss: 166062.9375\n",
            "Epoch 756/1000\n",
            "6/6 - 0s - loss: 39028.7852 - val_loss: 117207.7656\n",
            "Epoch 757/1000\n",
            "6/6 - 0s - loss: 33028.1641 - val_loss: 119802.8125\n",
            "Epoch 758/1000\n",
            "6/6 - 0s - loss: 27579.0586 - val_loss: 101601.2031\n",
            "Epoch 759/1000\n",
            "6/6 - 0s - loss: 25231.7637 - val_loss: 211465.1562\n",
            "Epoch 760/1000\n",
            "6/6 - 0s - loss: 27895.1816 - val_loss: 81220.7500\n",
            "Epoch 761/1000\n",
            "6/6 - 0s - loss: 17343.3496 - val_loss: 118923.1797\n",
            "Epoch 762/1000\n",
            "6/6 - 0s - loss: 17490.5898 - val_loss: 90883.1797\n",
            "Epoch 763/1000\n",
            "6/6 - 0s - loss: 17589.5664 - val_loss: 111321.6406\n",
            "Epoch 764/1000\n",
            "6/6 - 0s - loss: 14454.1309 - val_loss: 126277.6641\n",
            "Epoch 765/1000\n",
            "6/6 - 0s - loss: 12511.2285 - val_loss: 113483.6406\n",
            "Epoch 766/1000\n",
            "6/6 - 0s - loss: 13352.6514 - val_loss: 156036.7500\n",
            "Epoch 767/1000\n",
            "6/6 - 0s - loss: 12098.4990 - val_loss: 119785.6562\n",
            "Epoch 768/1000\n",
            "6/6 - 0s - loss: 12199.7832 - val_loss: 135369.7344\n",
            "Epoch 769/1000\n",
            "6/6 - 0s - loss: 12967.3232 - val_loss: 92757.4141\n",
            "Epoch 770/1000\n",
            "6/6 - 0s - loss: 14541.1006 - val_loss: 98852.2344\n",
            "Epoch 771/1000\n",
            "6/6 - 0s - loss: 14435.0771 - val_loss: 130192.3047\n",
            "Epoch 772/1000\n",
            "6/6 - 0s - loss: 12569.8975 - val_loss: 95575.5469\n",
            "Epoch 773/1000\n",
            "6/6 - 0s - loss: 11965.7783 - val_loss: 110426.0391\n",
            "Epoch 774/1000\n",
            "6/6 - 0s - loss: 11221.5039 - val_loss: 142413.8750\n",
            "Epoch 775/1000\n",
            "6/6 - 0s - loss: 10857.6777 - val_loss: 156834.9531\n",
            "Epoch 776/1000\n",
            "6/6 - 0s - loss: 10847.1787 - val_loss: 171919.4219\n",
            "Epoch 777/1000\n",
            "6/6 - 0s - loss: 11407.4414 - val_loss: 142230.2031\n",
            "Epoch 778/1000\n",
            "6/6 - 0s - loss: 11054.1992 - val_loss: 113259.6250\n",
            "Epoch 779/1000\n",
            "6/6 - 0s - loss: 12469.3105 - val_loss: 97678.9531\n",
            "Epoch 780/1000\n",
            "6/6 - 0s - loss: 14807.7490 - val_loss: 158557.3906\n",
            "Epoch 781/1000\n",
            "6/6 - 0s - loss: 13800.9287 - val_loss: 107589.5625\n",
            "Epoch 782/1000\n",
            "6/6 - 0s - loss: 14789.0391 - val_loss: 89233.2422\n",
            "Epoch 783/1000\n",
            "6/6 - 0s - loss: 19250.2461 - val_loss: 315596.5625\n",
            "Epoch 784/1000\n",
            "6/6 - 0s - loss: 28742.9668 - val_loss: 64647.6562\n",
            "Epoch 785/1000\n",
            "6/6 - 0s - loss: 37957.8711 - val_loss: 119914.8438\n",
            "Epoch 786/1000\n",
            "6/6 - 0s - loss: 118641.1328 - val_loss: 122379.2812\n",
            "Epoch 787/1000\n",
            "6/6 - 0s - loss: 73101.9453 - val_loss: 377262.8125\n",
            "Epoch 788/1000\n",
            "6/6 - 0s - loss: 82815.5156 - val_loss: 65705.1641\n",
            "Epoch 789/1000\n",
            "6/6 - 0s - loss: 126837.7969 - val_loss: 82492.8750\n",
            "Epoch 790/1000\n",
            "6/6 - 0s - loss: 105342.5547 - val_loss: 72446.1094\n",
            "Epoch 791/1000\n",
            "6/6 - 0s - loss: 54721.0352 - val_loss: 67848.5625\n",
            "Epoch 792/1000\n",
            "6/6 - 0s - loss: 42488.1055 - val_loss: 50011.9648\n",
            "Epoch 793/1000\n",
            "6/6 - 0s - loss: 36851.3320 - val_loss: 52708.5156\n",
            "Epoch 794/1000\n",
            "6/6 - 0s - loss: 35144.1562 - val_loss: 60507.1953\n",
            "Epoch 795/1000\n",
            "6/6 - 0s - loss: 33142.2539 - val_loss: 56682.0312\n",
            "Epoch 796/1000\n",
            "6/6 - 0s - loss: 33698.2539 - val_loss: 62449.6406\n",
            "Epoch 797/1000\n",
            "6/6 - 0s - loss: 28800.2148 - val_loss: 55036.2031\n",
            "Epoch 798/1000\n",
            "6/6 - 0s - loss: 26225.9082 - val_loss: 80185.5234\n",
            "Epoch 799/1000\n",
            "6/6 - 0s - loss: 25714.3984 - val_loss: 54083.1016\n",
            "Epoch 800/1000\n",
            "6/6 - 0s - loss: 33094.0312 - val_loss: 52241.5156\n",
            "Epoch 801/1000\n",
            "6/6 - 0s - loss: 53952.4297 - val_loss: 145369.0312\n",
            "Epoch 802/1000\n",
            "6/6 - 0s - loss: 210040.8750 - val_loss: 214285.3438\n",
            "Epoch 803/1000\n",
            "6/6 - 0s - loss: 230179.0312 - val_loss: 186222.6719\n",
            "Epoch 804/1000\n",
            "6/6 - 0s - loss: 160483.6250 - val_loss: 80739.7422\n",
            "Epoch 805/1000\n",
            "6/6 - 0s - loss: 125849.0312 - val_loss: 77509.1250\n",
            "Epoch 806/1000\n",
            "6/6 - 0s - loss: 79646.2500 - val_loss: 107662.7422\n",
            "Epoch 807/1000\n",
            "6/6 - 0s - loss: 102954.6484 - val_loss: 100257.6719\n",
            "Epoch 808/1000\n",
            "6/6 - 0s - loss: 57878.0273 - val_loss: 118541.1250\n",
            "Epoch 809/1000\n",
            "6/6 - 0s - loss: 52967.6992 - val_loss: 88632.0625\n",
            "Epoch 810/1000\n",
            "6/6 - 0s - loss: 30783.6543 - val_loss: 82655.1875\n",
            "Epoch 811/1000\n",
            "6/6 - 0s - loss: 28300.9688 - val_loss: 92676.8984\n",
            "Epoch 812/1000\n",
            "6/6 - 0s - loss: 26170.7012 - val_loss: 86401.5625\n",
            "Epoch 813/1000\n",
            "6/6 - 0s - loss: 27533.7012 - val_loss: 80232.2656\n",
            "Epoch 814/1000\n",
            "6/6 - 0s - loss: 42899.4375 - val_loss: 110196.7969\n",
            "Epoch 815/1000\n",
            "6/6 - 0s - loss: 158714.0156 - val_loss: 172412.3750\n",
            "Epoch 816/1000\n",
            "6/6 - 0s - loss: 157039.9375 - val_loss: 150171.2812\n",
            "Epoch 817/1000\n",
            "6/6 - 0s - loss: 68533.9219 - val_loss: 397956.6875\n",
            "Epoch 818/1000\n",
            "6/6 - 0s - loss: 56848.4922 - val_loss: 108903.9219\n",
            "Epoch 819/1000\n",
            "6/6 - 0s - loss: 48609.5625 - val_loss: 111518.8281\n",
            "Epoch 820/1000\n",
            "6/6 - 0s - loss: 37964.0508 - val_loss: 81508.4531\n",
            "Epoch 821/1000\n",
            "6/6 - 0s - loss: 27593.6113 - val_loss: 81190.4609\n",
            "Epoch 822/1000\n",
            "6/6 - 0s - loss: 24854.8652 - val_loss: 68862.2188\n",
            "Epoch 823/1000\n",
            "6/6 - 0s - loss: 27326.3535 - val_loss: 69465.5391\n",
            "Epoch 824/1000\n",
            "6/6 - 0s - loss: 20972.8438 - val_loss: 73705.6172\n",
            "Epoch 825/1000\n",
            "6/6 - 0s - loss: 19326.2461 - val_loss: 75764.3828\n",
            "Epoch 826/1000\n",
            "6/6 - 0s - loss: 16951.0898 - val_loss: 125766.2500\n",
            "Epoch 827/1000\n",
            "6/6 - 0s - loss: 16427.8555 - val_loss: 119680.1719\n",
            "Epoch 828/1000\n",
            "6/6 - 0s - loss: 14833.9775 - val_loss: 102370.9688\n",
            "Epoch 829/1000\n",
            "6/6 - 0s - loss: 14359.3154 - val_loss: 83757.6250\n",
            "Epoch 830/1000\n",
            "6/6 - 0s - loss: 14011.7588 - val_loss: 101751.8984\n",
            "Epoch 831/1000\n",
            "6/6 - 0s - loss: 13442.3486 - val_loss: 91443.5781\n",
            "Epoch 832/1000\n",
            "6/6 - 0s - loss: 12901.5117 - val_loss: 98307.7812\n",
            "Epoch 833/1000\n",
            "6/6 - 0s - loss: 14398.6680 - val_loss: 112246.7266\n",
            "Epoch 834/1000\n",
            "6/6 - 0s - loss: 14823.9053 - val_loss: 72264.8906\n",
            "Epoch 835/1000\n",
            "6/6 - 0s - loss: 13320.9707 - val_loss: 104864.3828\n",
            "Epoch 836/1000\n",
            "6/6 - 0s - loss: 13399.1104 - val_loss: 70565.8594\n",
            "Epoch 837/1000\n",
            "6/6 - 0s - loss: 13780.1152 - val_loss: 67403.7578\n",
            "Epoch 838/1000\n",
            "6/6 - 0s - loss: 14894.1377 - val_loss: 189270.7344\n",
            "Epoch 839/1000\n",
            "6/6 - 0s - loss: 14556.8086 - val_loss: 64410.1719\n",
            "Epoch 840/1000\n",
            "6/6 - 0s - loss: 12526.8193 - val_loss: 79689.3359\n",
            "Epoch 841/1000\n",
            "6/6 - 0s - loss: 12950.6953 - val_loss: 124322.2812\n",
            "Epoch 842/1000\n",
            "6/6 - 0s - loss: 15371.6982 - val_loss: 70489.2891\n",
            "Epoch 843/1000\n",
            "6/6 - 0s - loss: 16700.9570 - val_loss: 71184.6484\n",
            "Epoch 844/1000\n",
            "6/6 - 0s - loss: 14698.6006 - val_loss: 203944.5469\n",
            "Epoch 845/1000\n",
            "6/6 - 0s - loss: 12162.1787 - val_loss: 74720.6172\n",
            "Epoch 846/1000\n",
            "6/6 - 0s - loss: 11003.4014 - val_loss: 82264.1172\n",
            "Epoch 847/1000\n",
            "6/6 - 0s - loss: 13032.2627 - val_loss: 55975.5039\n",
            "Epoch 848/1000\n",
            "6/6 - 0s - loss: 20965.1426 - val_loss: 1200555.6250\n",
            "Epoch 849/1000\n",
            "6/6 - 0s - loss: 14538.3086 - val_loss: 545303.0000\n",
            "Epoch 850/1000\n",
            "6/6 - 0s - loss: 22707.4512 - val_loss: 80439.4453\n",
            "Epoch 851/1000\n",
            "6/6 - 0s - loss: 43302.1680 - val_loss: 59011.7734\n",
            "Epoch 852/1000\n",
            "6/6 - 0s - loss: 37246.5547 - val_loss: 66725.9688\n",
            "Epoch 853/1000\n",
            "6/6 - 0s - loss: 31708.9922 - val_loss: 66197.4531\n",
            "Epoch 854/1000\n",
            "6/6 - 0s - loss: 22992.9688 - val_loss: 148395.0156\n",
            "Epoch 855/1000\n",
            "6/6 - 0s - loss: 20212.9863 - val_loss: 127862.5469\n",
            "Epoch 856/1000\n",
            "6/6 - 0s - loss: 17812.4531 - val_loss: 122230.8203\n",
            "Epoch 857/1000\n",
            "6/6 - 0s - loss: 20142.9141 - val_loss: 93189.8750\n",
            "Epoch 858/1000\n",
            "6/6 - 0s - loss: 20825.2012 - val_loss: 86451.7188\n",
            "Epoch 859/1000\n",
            "6/6 - 0s - loss: 16826.1602 - val_loss: 70764.1094\n",
            "Epoch 860/1000\n",
            "6/6 - 0s - loss: 16194.8350 - val_loss: 107551.6562\n",
            "Epoch 861/1000\n",
            "6/6 - 0s - loss: 17751.4590 - val_loss: 82155.9141\n",
            "Epoch 862/1000\n",
            "6/6 - 0s - loss: 17049.2852 - val_loss: 105333.3359\n",
            "Epoch 863/1000\n",
            "6/6 - 0s - loss: 17609.3184 - val_loss: 187906.6719\n",
            "Epoch 864/1000\n",
            "6/6 - 0s - loss: 20291.6992 - val_loss: 146291.4531\n",
            "Epoch 865/1000\n",
            "6/6 - 0s - loss: 19041.4863 - val_loss: 107307.7188\n",
            "Epoch 866/1000\n",
            "6/6 - 0s - loss: 19405.2754 - val_loss: 105296.3203\n",
            "Epoch 867/1000\n",
            "6/6 - 0s - loss: 33127.7578 - val_loss: 109379.6406\n",
            "Epoch 868/1000\n",
            "6/6 - 0s - loss: 80732.4531 - val_loss: 61832.3594\n",
            "Epoch 869/1000\n",
            "6/6 - 0s - loss: 62483.2930 - val_loss: 65470.3047\n",
            "Epoch 870/1000\n",
            "6/6 - 0s - loss: 34399.8594 - val_loss: 153384.8438\n",
            "Epoch 871/1000\n",
            "6/6 - 0s - loss: 32715.7734 - val_loss: 130432.9062\n",
            "Epoch 872/1000\n",
            "6/6 - 0s - loss: 76707.8438 - val_loss: 110788.9844\n",
            "Epoch 873/1000\n",
            "6/6 - 0s - loss: 49044.0586 - val_loss: 74928.6562\n",
            "Epoch 874/1000\n",
            "6/6 - 0s - loss: 23773.6270 - val_loss: 62230.7930\n",
            "Epoch 875/1000\n",
            "6/6 - 0s - loss: 22455.1270 - val_loss: 61549.0234\n",
            "Epoch 876/1000\n",
            "6/6 - 0s - loss: 45062.5977 - val_loss: 82925.1719\n",
            "Epoch 877/1000\n",
            "6/6 - 0s - loss: 99684.6172 - val_loss: 70191.8672\n",
            "Epoch 878/1000\n",
            "6/6 - 0s - loss: 199397.9219 - val_loss: 930368.7500\n",
            "Epoch 879/1000\n",
            "6/6 - 0s - loss: 71276.0078 - val_loss: 136522.2812\n",
            "Epoch 880/1000\n",
            "6/6 - 0s - loss: 526088.0000 - val_loss: 167960.1406\n",
            "Epoch 881/1000\n",
            "6/6 - 0s - loss: 793872.1250 - val_loss: 87411.2344\n",
            "Epoch 882/1000\n",
            "6/6 - 0s - loss: 154988.1719 - val_loss: 147401.8438\n",
            "Epoch 883/1000\n",
            "6/6 - 0s - loss: 256574.7812 - val_loss: 150334.1562\n",
            "Epoch 884/1000\n",
            "6/6 - 0s - loss: 209102.9688 - val_loss: 129268.6094\n",
            "Epoch 885/1000\n",
            "6/6 - 0s - loss: 142661.9531 - val_loss: 108733.7812\n",
            "Epoch 886/1000\n",
            "6/6 - 0s - loss: 106194.8594 - val_loss: 96843.2109\n",
            "Epoch 887/1000\n",
            "6/6 - 0s - loss: 117976.4844 - val_loss: 151233.1719\n",
            "Epoch 888/1000\n",
            "6/6 - 0s - loss: 83096.4297 - val_loss: 103632.6328\n",
            "Epoch 889/1000\n",
            "6/6 - 0s - loss: 70784.3281 - val_loss: 69431.2812\n",
            "Epoch 890/1000\n",
            "6/6 - 0s - loss: 57689.2539 - val_loss: 63246.1914\n",
            "Epoch 891/1000\n",
            "6/6 - 0s - loss: 43046.1445 - val_loss: 54572.0117\n",
            "Epoch 892/1000\n",
            "6/6 - 0s - loss: 32647.0703 - val_loss: 50407.2422\n",
            "Epoch 893/1000\n",
            "6/6 - 0s - loss: 29532.1367 - val_loss: 51515.2344\n",
            "Epoch 894/1000\n",
            "6/6 - 0s - loss: 31234.3535 - val_loss: 49864.9141\n",
            "Epoch 895/1000\n",
            "6/6 - 0s - loss: 32449.1309 - val_loss: 82787.5859\n",
            "Epoch 896/1000\n",
            "6/6 - 0s - loss: 31047.1582 - val_loss: 51429.8906\n",
            "Epoch 897/1000\n",
            "6/6 - 0s - loss: 40934.7656 - val_loss: 73694.3594\n",
            "Epoch 898/1000\n",
            "6/6 - 0s - loss: 22883.7559 - val_loss: 65403.8125\n",
            "Epoch 899/1000\n",
            "6/6 - 0s - loss: 20537.8418 - val_loss: 64473.3359\n",
            "Epoch 900/1000\n",
            "6/6 - 0s - loss: 18974.7500 - val_loss: 54965.9961\n",
            "Epoch 901/1000\n",
            "6/6 - 0s - loss: 17120.0820 - val_loss: 58640.8047\n",
            "Epoch 902/1000\n",
            "6/6 - 0s - loss: 16865.3477 - val_loss: 52757.7734\n",
            "Epoch 903/1000\n",
            "6/6 - 0s - loss: 18341.0000 - val_loss: 51203.4062\n",
            "Epoch 904/1000\n",
            "6/6 - 0s - loss: 14400.0986 - val_loss: 50277.1016\n",
            "Epoch 905/1000\n",
            "6/6 - 0s - loss: 13482.8027 - val_loss: 57236.1328\n",
            "Epoch 906/1000\n",
            "6/6 - 0s - loss: 24841.3301 - val_loss: 50079.2383\n",
            "Epoch 907/1000\n",
            "6/6 - 0s - loss: 33388.8086 - val_loss: 88982.1719\n",
            "Epoch 908/1000\n",
            "6/6 - 0s - loss: 153980.9219 - val_loss: 120781.7344\n",
            "Epoch 909/1000\n",
            "6/6 - 0s - loss: 114248.5469 - val_loss: 60501.2344\n",
            "Epoch 910/1000\n",
            "6/6 - 0s - loss: 55190.0195 - val_loss: 45791.1875\n",
            "Epoch 911/1000\n",
            "6/6 - 0s - loss: 87481.2266 - val_loss: 78666.3828\n",
            "Epoch 912/1000\n",
            "6/6 - 0s - loss: 38972.7852 - val_loss: 53148.4258\n",
            "Epoch 913/1000\n",
            "6/6 - 0s - loss: 50329.1328 - val_loss: 50998.7500\n",
            "Epoch 914/1000\n",
            "6/6 - 0s - loss: 47100.2031 - val_loss: 98140.4688\n",
            "Epoch 915/1000\n",
            "6/6 - 0s - loss: 151588.4375 - val_loss: 131150.7969\n",
            "Epoch 916/1000\n",
            "6/6 - 0s - loss: 133513.8125 - val_loss: 86504.8906\n",
            "Epoch 917/1000\n",
            "6/6 - 0s - loss: 76981.0156 - val_loss: 81965.5078\n",
            "Epoch 918/1000\n",
            "6/6 - 0s - loss: 53980.7188 - val_loss: 73576.5234\n",
            "Epoch 919/1000\n",
            "6/6 - 0s - loss: 45015.6523 - val_loss: 68238.5703\n",
            "Epoch 920/1000\n",
            "6/6 - 0s - loss: 69343.5781 - val_loss: 79480.3281\n",
            "Epoch 921/1000\n",
            "6/6 - 0s - loss: 51943.8945 - val_loss: 186145.5938\n",
            "Epoch 922/1000\n",
            "6/6 - 0s - loss: 142691.1406 - val_loss: 211337.9062\n",
            "Epoch 923/1000\n",
            "6/6 - 0s - loss: 290662.2812 - val_loss: 199310.0000\n",
            "Epoch 924/1000\n",
            "6/6 - 0s - loss: 255996.7500 - val_loss: 166660.7812\n",
            "Epoch 925/1000\n",
            "6/6 - 0s - loss: 161608.8125 - val_loss: 111262.9297\n",
            "Epoch 926/1000\n",
            "6/6 - 0s - loss: 94120.2969 - val_loss: 121246.2812\n",
            "Epoch 927/1000\n",
            "6/6 - 0s - loss: 78987.1797 - val_loss: 55798.8320\n",
            "Epoch 928/1000\n",
            "6/6 - 0s - loss: 42755.4180 - val_loss: 66819.0781\n",
            "Epoch 929/1000\n",
            "6/6 - 0s - loss: 38434.4414 - val_loss: 76231.6406\n",
            "Epoch 930/1000\n",
            "6/6 - 0s - loss: 36986.3633 - val_loss: 76284.7422\n",
            "Epoch 931/1000\n",
            "6/6 - 0s - loss: 26687.1836 - val_loss: 75030.1953\n",
            "Epoch 932/1000\n",
            "6/6 - 0s - loss: 18720.4277 - val_loss: 62025.1094\n",
            "Epoch 933/1000\n",
            "6/6 - 0s - loss: 18116.6621 - val_loss: 68904.4609\n",
            "Epoch 934/1000\n",
            "6/6 - 0s - loss: 17679.0098 - val_loss: 55527.2969\n",
            "Epoch 935/1000\n",
            "6/6 - 0s - loss: 15811.0410 - val_loss: 115786.8594\n",
            "Epoch 936/1000\n",
            "6/6 - 0s - loss: 16428.3574 - val_loss: 51584.0977\n",
            "Epoch 937/1000\n",
            "6/6 - 0s - loss: 17151.5820 - val_loss: 60892.0117\n",
            "Epoch 938/1000\n",
            "6/6 - 0s - loss: 15644.5088 - val_loss: 53875.2969\n",
            "Epoch 939/1000\n",
            "6/6 - 0s - loss: 13961.2900 - val_loss: 73617.2578\n",
            "Epoch 940/1000\n",
            "6/6 - 0s - loss: 15207.1992 - val_loss: 55053.1953\n",
            "Epoch 941/1000\n",
            "6/6 - 0s - loss: 14476.1035 - val_loss: 56264.9648\n",
            "Epoch 942/1000\n",
            "6/6 - 0s - loss: 14094.7031 - val_loss: 57429.8477\n",
            "Epoch 943/1000\n",
            "6/6 - 0s - loss: 13199.0723 - val_loss: 60524.4922\n",
            "Epoch 944/1000\n",
            "6/6 - 0s - loss: 12839.4727 - val_loss: 53598.3516\n",
            "Epoch 945/1000\n",
            "6/6 - 0s - loss: 12560.1768 - val_loss: 62098.3477\n",
            "Epoch 946/1000\n",
            "6/6 - 0s - loss: 12573.1416 - val_loss: 51725.1523\n",
            "Epoch 947/1000\n",
            "6/6 - 0s - loss: 12245.8711 - val_loss: 54759.4141\n",
            "Epoch 948/1000\n",
            "6/6 - 0s - loss: 11782.2070 - val_loss: 54556.5938\n",
            "Epoch 949/1000\n",
            "6/6 - 0s - loss: 11245.3193 - val_loss: 59045.6953\n",
            "Epoch 950/1000\n",
            "6/6 - 0s - loss: 11492.7520 - val_loss: 51578.5469\n",
            "Epoch 951/1000\n",
            "6/6 - 0s - loss: 12186.1650 - val_loss: 47617.7930\n",
            "Epoch 952/1000\n",
            "6/6 - 0s - loss: 12437.7969 - val_loss: 151161.4531\n",
            "Epoch 953/1000\n",
            "6/6 - 0s - loss: 16820.4590 - val_loss: 48328.6484\n",
            "Epoch 954/1000\n",
            "6/6 - 0s - loss: 16042.1182 - val_loss: 47917.1719\n",
            "Epoch 955/1000\n",
            "6/6 - 0s - loss: 19009.1738 - val_loss: 57060.4023\n",
            "Epoch 956/1000\n",
            "6/6 - 0s - loss: 18143.5820 - val_loss: 48499.5234\n",
            "Epoch 957/1000\n",
            "6/6 - 0s - loss: 28796.4629 - val_loss: 66112.7188\n",
            "Epoch 958/1000\n",
            "6/6 - 0s - loss: 26218.8613 - val_loss: 47660.9102\n",
            "Epoch 959/1000\n",
            "6/6 - 0s - loss: 26702.5039 - val_loss: 111749.6016\n",
            "Epoch 960/1000\n",
            "6/6 - 0s - loss: 31473.9492 - val_loss: 55224.5859\n",
            "Epoch 961/1000\n",
            "6/6 - 0s - loss: 30308.6270 - val_loss: 75734.1719\n",
            "Epoch 962/1000\n",
            "6/6 - 0s - loss: 41486.1719 - val_loss: 110859.7266\n",
            "Epoch 963/1000\n",
            "6/6 - 0s - loss: 138243.0000 - val_loss: 90517.3125\n",
            "Epoch 964/1000\n",
            "6/6 - 0s - loss: 84341.5000 - val_loss: 171655.5781\n",
            "Epoch 965/1000\n",
            "6/6 - 0s - loss: 65951.6328 - val_loss: 67682.6016\n",
            "Epoch 966/1000\n",
            "6/6 - 0s - loss: 40256.1719 - val_loss: 81924.3438\n",
            "Epoch 967/1000\n",
            "6/6 - 0s - loss: 40749.2930 - val_loss: 72018.9219\n",
            "Epoch 968/1000\n",
            "6/6 - 0s - loss: 37064.6367 - val_loss: 73410.7031\n",
            "Epoch 969/1000\n",
            "6/6 - 0s - loss: 26534.6016 - val_loss: 73608.1875\n",
            "Epoch 970/1000\n",
            "6/6 - 0s - loss: 33345.4453 - val_loss: 73750.5781\n",
            "Epoch 971/1000\n",
            "6/6 - 0s - loss: 23257.1152 - val_loss: 69792.7344\n",
            "Epoch 972/1000\n",
            "6/6 - 0s - loss: 17974.7129 - val_loss: 69158.0938\n",
            "Epoch 973/1000\n",
            "6/6 - 0s - loss: 16527.5918 - val_loss: 89910.6016\n",
            "Epoch 974/1000\n",
            "6/6 - 0s - loss: 16057.1230 - val_loss: 97340.6406\n",
            "Epoch 975/1000\n",
            "6/6 - 0s - loss: 15446.5762 - val_loss: 61458.7891\n",
            "Epoch 976/1000\n",
            "6/6 - 0s - loss: 12993.2021 - val_loss: 60724.5039\n",
            "Epoch 977/1000\n",
            "6/6 - 0s - loss: 13091.0449 - val_loss: 57110.5430\n",
            "Epoch 978/1000\n",
            "6/6 - 0s - loss: 12524.5947 - val_loss: 61175.6758\n",
            "Epoch 979/1000\n",
            "6/6 - 0s - loss: 11186.8066 - val_loss: 58789.4688\n",
            "Epoch 980/1000\n",
            "6/6 - 0s - loss: 12797.7109 - val_loss: 60465.1172\n",
            "Epoch 981/1000\n",
            "6/6 - 0s - loss: 12588.0752 - val_loss: 59158.3281\n",
            "Epoch 982/1000\n",
            "6/6 - 0s - loss: 11437.0996 - val_loss: 54394.2891\n",
            "Epoch 983/1000\n",
            "6/6 - 0s - loss: 10572.0693 - val_loss: 57053.8438\n",
            "Epoch 984/1000\n",
            "6/6 - 0s - loss: 10716.1309 - val_loss: 49384.9609\n",
            "Epoch 985/1000\n",
            "6/6 - 0s - loss: 11775.5986 - val_loss: 57092.1250\n",
            "Epoch 986/1000\n",
            "6/6 - 0s - loss: 10311.6963 - val_loss: 50398.0820\n",
            "Epoch 987/1000\n",
            "6/6 - 0s - loss: 10200.8027 - val_loss: 48110.7891\n",
            "Epoch 988/1000\n",
            "6/6 - 0s - loss: 10623.6982 - val_loss: 48010.4727\n",
            "Epoch 989/1000\n",
            "6/6 - 0s - loss: 17215.4980 - val_loss: 58603.8672\n",
            "Epoch 990/1000\n",
            "6/6 - 0s - loss: 17569.6953 - val_loss: 53965.3477\n",
            "Epoch 991/1000\n",
            "6/6 - 0s - loss: 14490.1006 - val_loss: 80820.3125\n",
            "Epoch 992/1000\n",
            "6/6 - 0s - loss: 17843.7461 - val_loss: 61833.2656\n",
            "Epoch 993/1000\n",
            "6/6 - 0s - loss: 23314.4629 - val_loss: 280298.2500\n",
            "Epoch 994/1000\n",
            "6/6 - 0s - loss: 42347.6641 - val_loss: 77277.6094\n",
            "Epoch 995/1000\n",
            "6/6 - 0s - loss: 41494.8086 - val_loss: 81648.1719\n",
            "Epoch 996/1000\n",
            "6/6 - 0s - loss: 40372.5781 - val_loss: 63815.4844\n",
            "Epoch 997/1000\n",
            "6/6 - 0s - loss: 67759.8516 - val_loss: 63660.5469\n",
            "Epoch 998/1000\n",
            "6/6 - 0s - loss: 28859.5625 - val_loss: 55248.7852\n",
            "Epoch 999/1000\n",
            "6/6 - 0s - loss: 31678.4629 - val_loss: 52278.7031\n",
            "Epoch 1000/1000\n",
            "6/6 - 0s - loss: 23875.2070 - val_loss: 55646.6953\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe2994fe750>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWFjI2HNbW-n"
      },
      "source": [
        "# Mean Square Error\n",
        "\n",
        "The mean square error is the sum of the squared differences between the prediction ($\\hat{y}$) and the expected ($y$).  MSE values are not of a particular unit.  If an MSE value has decreased for a model, that is good.  However, beyond this, there is not much more you can determine.  Low MSE values are desired.\n",
        "\n",
        "$ \\mbox{MSE} = \\frac{1}{n} \\sum_{i=1}^n \\left(\\hat{y}_i - y_i\\right)^2 $\n",
        "\n",
        "\n",
        "# Root Mean Square Error\n",
        "\n",
        "The root mean square (RMSE) is essentially the square root of the MSE.  Because of this, the RMSE error is in the same units as the training data outcome. Low RMSE values are desired.\n",
        "\n",
        "$ \\mbox{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n \\left(\\hat{y}_i - y_i\\right)^2} $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xE7r0D4sbW-m",
        "outputId": "3ebad00d-3cf6-4ced-b38b-d52ac85a3960"
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "pred = model.predict(x_test)\n",
        "\n",
        "score = metrics.mean_squared_error(pred,y_test)\n",
        "print(\"Final score (MSE): {}\".format(score))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final score (MSE): 55646.6953125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBcIz4QrbW-n",
        "outputId": "ac483f14-010f-478b-a315-0b00e06b754c"
      },
      "source": [
        "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
        "print(\"Final score (RMSE): {}\".format(score))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final score (RMSE): 235.89552307128906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05hOQT3wbW-o"
      },
      "source": [
        "def chart_regression(pred, y, sort=True):\n",
        "    t = pd.DataFrame({'pred': pred, 'y': y.flatten()})\n",
        "    if sort: t.sort_values(by=['y'], inplace=True)\n",
        "    plt.plot(t['y'].tolist(), label='expected')\n",
        "    plt.plot(t['pred'].tolist(), label='prediction')\n",
        "    plt.ylabel('output')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "HDTnddFEbW-o",
        "outputId": "5fdc9bdf-90fb-4395-c83d-f036cc4cce3c"
      },
      "source": [
        "chart_regression(pred.flatten(),y_test)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDzggvc2bW-p"
      },
      "source": [
        "model.save('model_final.h5')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "r4myQevbh62-",
        "outputId": "5a3d6f4c-714d-485f-c724-331121beae39"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('model_final.h5')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_c308e084-9475-40e6-b85d-8f6e07161d80\", \"model_final.h5\", 260512)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNPsCeBziMm_",
        "outputId": "12038254-50b4-4494-f8a9-8d9a84a0f0f3"
      },
      "source": [
        "!pip install tensorflowjs"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflowjs in /usr/local/lib/python3.7/dist-packages (3.9.0)\n",
            "Requirement already satisfied: six<2,>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflowjs) (1.15.0)\n",
            "Requirement already satisfied: tensorflow<3,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflowjs) (2.6.0)\n",
            "Requirement already satisfied: tensorflow-hub<0.13,>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflowjs) (0.12.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.37.0)\n",
            "Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (5.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (3.3.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.12)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (2.6.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.4.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.41.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.1.2)\n",
            "Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (2.6.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.12.0)\n",
            "Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (2.6.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (3.1.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.6.3)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.12.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (3.7.4.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.1.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.19.5)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow<3,>=2.1.0->tensorflowjs) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (1.8.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (2021.5.30)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGsur3XriP4W"
      },
      "source": [
        "import tensorflowjs as tfjs"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UbQAl9fiQ3I"
      },
      "source": [
        "tfjs.converters.save_keras_model(model, 'model_final')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPhZBzXUinxz",
        "outputId": "9693c06f-f5d6-4898-de66-11483b085104"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 128)               6656      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 17,537\n",
            "Trainable params: 17,537\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}